# TradeMaster Docker æ•…éšœæ’é™¤æŒ‡å—

<div align="center">
    <h2>ğŸ”§ ç³»ç»Ÿæ€§æ•…éšœè¯Šæ–­ä¸è§£å†³æ–¹æ¡ˆ</h2>
    <p>å¿«é€Ÿå®šä½é—®é¢˜ï¼Œé«˜æ•ˆè§£å†³æ•…éšœ</p>
</div>

---

## ğŸ“‹ ç›®å½•

- [ğŸš¨ ç´§æ€¥æ•…éšœå¿«é€Ÿå¤„ç†](#ç´§æ€¥æ•…éšœå¿«é€Ÿå¤„ç†)
- [ğŸ” ç³»ç»Ÿæ€§æ•…éšœè¯Šæ–­](#ç³»ç»Ÿæ€§æ•…éšœè¯Šæ–­)
- [âš¡ æ€§èƒ½é—®é¢˜åˆ†æ](#æ€§èƒ½é—®é¢˜åˆ†æ)
- [ğŸ› å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ](#å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ)
- [ğŸ“Š ç›‘æ§å’Œæ—¥å¿—åˆ†æ](#ç›‘æ§å’Œæ—¥å¿—åˆ†æ)
- [ğŸ› ï¸ è°ƒè¯•å·¥å…·ç®±](#è°ƒè¯•å·¥å…·ç®±)
- [ğŸ”„ æ¢å¤å’Œä¿®å¤æµç¨‹](#æ¢å¤å’Œä¿®å¤æµç¨‹)

---

## ğŸš¨ ç´§æ€¥æ•…éšœå¿«é€Ÿå¤„ç†

### âš ï¸ ç”Ÿäº§ç¯å¢ƒç´§æ€¥æ¢å¤

å½“ç”Ÿäº§ç¯å¢ƒå‡ºç°ä¸¥é‡æ•…éšœæ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§å¤„ç†ï¼š

#### ğŸ”¥ P0çº§æ•…éšœ (æœåŠ¡å®Œå…¨ä¸å¯ç”¨)

**1åˆ†é’Ÿå†…å¿«é€Ÿè¯Šæ–­**:
```bash
# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker ps -a | grep trademaster

# æ£€æŸ¥ç³»ç»Ÿèµ„æº
df -h && free -h && top -n1

# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
curl -I http://localhost:8080/health
```

**5åˆ†é’Ÿå†…ç´§æ€¥æ¢å¤**:
```bash
# æ–¹æ¡ˆ1: å¿«é€Ÿé‡å¯
docker restart trademaster-container

# æ–¹æ¡ˆ2: å›æ»šåˆ°å¤‡ä»½
docker stop trademaster-container
docker run -d --name trademaster-container-backup \
  # ... ä½¿ç”¨ä¸Šä¸€ä¸ªç¨³å®šç‰ˆæœ¬çš„é•œåƒ

# æ–¹æ¡ˆ3: å¯ç”¨å¤‡ç”¨å®ä¾‹
docker start trademaster-container-standby
# åˆ‡æ¢è´Ÿè½½å‡è¡¡åˆ°å¤‡ç”¨å®ä¾‹
```

#### ğŸŸ¡ P1çº§æ•…éšœ (éƒ¨åˆ†åŠŸèƒ½ä¸å¯ç”¨)

**å¿«é€Ÿè¯Šæ–­è„šæœ¬**:
```bash
#!/bin/bash
# emergency_diagnosis.sh

echo "=== ç´§æ€¥è¯Šæ–­æŠ¥å‘Š $(date) ==="

echo "1. å®¹å™¨çŠ¶æ€:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

echo "2. èµ„æºä½¿ç”¨:"
docker stats --no-stream trademaster-container

echo "3. æœ€è¿‘é”™è¯¯æ—¥å¿—:"
docker logs --tail 50 trademaster-container | grep -i error

echo "4. ç½‘ç»œè¿é€šæ€§:"
for port in 8080 8888 5001; do
    echo "Port $port: $(curl -s -o /dev/null -w "%{http_code}" http://localhost:$port || echo "FAILED")"
done

echo "5. ç£ç›˜ç©ºé—´:"
df -h | grep -E "(/$|/var|/opt)"

echo "6. è¿›ç¨‹çŠ¶æ€:"
docker exec trademaster-container ps aux | head -10

echo "=== è¯Šæ–­å®Œæˆ ==="
```

### ğŸ”„ è‡ªåŠ¨æ¢å¤æœºåˆ¶

#### å¥åº·æ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# health_check.sh

CONTAINER_NAME="trademaster-container"
MAX_RETRIES=3
RETRY_COUNT=0

check_health() {
    # æ£€æŸ¥å®¹å™¨æ˜¯å¦è¿è¡Œ
    if ! docker ps | grep -q $CONTAINER_NAME; then
        return 1
    fi
    
    # æ£€æŸ¥HTTPæœåŠ¡
    if ! curl -sf http://localhost:8080/health > /dev/null; then
        return 1
    fi
    
    # æ£€æŸ¥å…³é”®è¿›ç¨‹
    if ! docker exec $CONTAINER_NAME pgrep python > /dev/null; then
        return 1
    fi
    
    return 0
}

restart_service() {
    echo "$(date): å°è¯•é‡å¯æœåŠ¡... (é‡è¯•æ¬¡æ•°: $RETRY_COUNT/$MAX_RETRIES)"
    
    docker restart $CONTAINER_NAME
    sleep 30
    
    if check_health; then
        echo "$(date): æœåŠ¡é‡å¯æˆåŠŸ"
        # å‘é€æ¢å¤é€šçŸ¥
        curl -X POST https://hooks.slack.com/your-webhook \
            -H 'Content-type: application/json' \
            --data '{"text":"TradeMasteræœåŠ¡å·²è‡ªåŠ¨æ¢å¤"}'
        exit 0
    else
        RETRY_COUNT=$((RETRY_COUNT + 1))
        if [ $RETRY_COUNT -ge $MAX_RETRIES ]; then
            echo "$(date): è‡ªåŠ¨æ¢å¤å¤±è´¥ï¼Œéœ€è¦äººå·¥ä»‹å…¥"
            # å‘é€å‘Šè­¦
            curl -X POST https://hooks.slack.com/your-webhook \
                -H 'Content-type: application/json' \
                --data '{"text":"TradeMasteræœåŠ¡è‡ªåŠ¨æ¢å¤å¤±è´¥ï¼Œéœ€è¦äººå·¥ä»‹å…¥"}'
            exit 1
        fi
    fi
}

# ä¸»é€»è¾‘
if ! check_health; then
    restart_service
else
    echo "$(date): æœåŠ¡è¿è¡Œæ­£å¸¸"
fi
```

---

## ğŸ” ç³»ç»Ÿæ€§æ•…éšœè¯Šæ–­

### ğŸ—ï¸ è¯Šæ–­å†³ç­–æ ‘

```
æ•…éšœæŠ¥å‘Š
    â”œâ”€ æ— æ³•è®¿é—®æœåŠ¡
    â”‚   â”œâ”€ å®¹å™¨æœªè¿è¡Œ â†’ [å®¹å™¨å¯åŠ¨æ•…éšœ]
    â”‚   â”œâ”€ ç«¯å£æ— å“åº” â†’ [ç½‘ç»œé…ç½®é—®é¢˜]
    â”‚   â””â”€ æœåŠ¡å¼‚å¸¸ â†’ [åº”ç”¨å±‚æ•…éšœ]
    â”‚
    â”œâ”€ æ€§èƒ½ç¼“æ…¢
    â”‚   â”œâ”€ CPUé«˜å ç”¨ â†’ [è®¡ç®—èµ„æºé—®é¢˜]
    â”‚   â”œâ”€ å†…å­˜ä¸è¶³ â†’ [å†…å­˜èµ„æºé—®é¢˜]
    â”‚   â””â”€ I/Oé˜»å¡ â†’ [å­˜å‚¨æ€§èƒ½é—®é¢˜]
    â”‚
    â””â”€ åŠŸèƒ½å¼‚å¸¸
        â”œâ”€ æ¨¡å—å¯¼å…¥å¤±è´¥ â†’ [ç¯å¢ƒé…ç½®é—®é¢˜]
        â”œâ”€ æ•°æ®è®¿é—®é”™è¯¯ â†’ [æ•°æ®å·é—®é¢˜]
        â””â”€ è®¡ç®—ç»“æœå¼‚å¸¸ â†’ [ä¸šåŠ¡é€»è¾‘é—®é¢˜]
```

### ğŸ” åˆ†å±‚è¯Šæ–­æ–¹æ³•

#### 1. åŸºç¡€è®¾æ–½å±‚è¯Šæ–­

**Dockerç¯å¢ƒæ£€æŸ¥**:
```bash
# DockerçŠ¶æ€æ£€æŸ¥è„šæœ¬
#!/bin/bash
# docker_diagnosis.sh

echo "=== Dockerç¯å¢ƒè¯Šæ–­ ==="

echo "1. Dockerç‰ˆæœ¬ä¿¡æ¯:"
docker version

echo "2. Dockerç³»ç»Ÿä¿¡æ¯:"
docker system df
docker system info | grep -E "(Storage Driver|Logging Driver|Cgroup Driver)"

echo "3. ç½‘ç»œé…ç½®:"
docker network ls
docker network inspect bridge | jq '.[0].IPAM'

echo "4. å­˜å‚¨ä½¿ç”¨:"
docker system df -v

echo "5. æ­£åœ¨è¿è¡Œçš„å®¹å™¨:"
docker ps --format "table {{.Names}}\t{{.Image}}\t{{.Status}}\t{{.Ports}}"

echo "6. èµ„æºé™åˆ¶:"
docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"
```

**ç³»ç»Ÿèµ„æºæ£€æŸ¥**:
```bash
#!/bin/bash
# system_diagnosis.sh

echo "=== ç³»ç»Ÿèµ„æºè¯Šæ–­ ==="

echo "1. CPUä¿¡æ¯:"
lscpu | grep -E "(Model name|CPU\(s\)|Thread)"
top -bn1 | grep "Cpu(s)"

echo "2. å†…å­˜ä¿¡æ¯:"
free -h
cat /proc/meminfo | grep -E "(MemTotal|MemFree|MemAvailable|Cached)"

echo "3. ç£ç›˜ä¿¡æ¯:"
df -h
lsblk
iostat -x 1 1

echo "4. ç½‘ç»œä¿¡æ¯:"
netstat -tuln | grep -E "(8080|8888|5001)"
ss -tuln | grep -E "(8080|8888|5001)"

echo "5. è¿›ç¨‹ä¿¡æ¯:"
ps aux | grep -E "(docker|python)" | head -10
```

#### 2. å®¹å™¨å±‚è¯Šæ–­

**å®¹å™¨å†…éƒ¨æ£€æŸ¥**:
```bash
#!/bin/bash
# container_diagnosis.sh

CONTAINER_NAME="trademaster-container"

echo "=== å®¹å™¨å†…éƒ¨è¯Šæ–­ ==="

echo "1. å®¹å™¨é…ç½®ä¿¡æ¯:"
docker inspect $CONTAINER_NAME | jq '.[] | {
    Image: .Config.Image,
    Env: .Config.Env,
    Cmd: .Config.Cmd,
    Mounts: .Mounts,
    NetworkSettings: .NetworkSettings.Ports
}'

echo "2. å®¹å™¨èµ„æºä½¿ç”¨:"
docker exec $CONTAINER_NAME bash -c "
echo 'CPUæ ¸å¿ƒæ•°: $(nproc)'
echo 'å†…å­˜ä¿¡æ¯:'
free -h
echo 'ç£ç›˜ä½¿ç”¨:'
df -h
echo 'è¿›ç¨‹åˆ—è¡¨:'
ps aux | head -10
"

echo "3. Pythonç¯å¢ƒæ£€æŸ¥:"
docker exec $CONTAINER_NAME bash -c "
echo 'Pythonç‰ˆæœ¬:'
python3 --version
echo 'pipç‰ˆæœ¬:'
pip --version
echo 'è™šæ‹Ÿç¯å¢ƒ:'
echo \$VIRTUAL_ENV
echo 'Pythonè·¯å¾„:'
python3 -c 'import sys; print(\"\\n\".join(sys.path))'
"

echo "4. å…³é”®æœåŠ¡çŠ¶æ€:"
docker exec $CONTAINER_NAME bash -c "
echo 'ç«¯å£ç›‘å¬:'
netstat -tuln | grep -E ':(8080|8888|5000)'
echo 'Pythonè¿›ç¨‹:'
pgrep -fl python
"
```

#### 3. åº”ç”¨å±‚è¯Šæ–­

**TradeMasterç¯å¢ƒæ£€æŸ¥**:
```bash
#!/bin/bash
# app_diagnosis.sh

CONTAINER_NAME="trademaster-container"

echo "=== åº”ç”¨å±‚è¯Šæ–­ ==="

echo "1. TradeMasteræ¨¡å—æ£€æŸ¥:"
docker exec $CONTAINER_NAME python3 -c "
try:
    import trademaster
    print('âœ… TradeMasteræ¨¡å—å¯¼å…¥æˆåŠŸ')
    print('ç‰ˆæœ¬ä¿¡æ¯:', getattr(trademaster, '__version__', 'æœªçŸ¥'))
except Exception as e:
    print('âŒ TradeMasteræ¨¡å—å¯¼å…¥å¤±è´¥:', str(e))
    import traceback
    traceback.print_exc()
"

echo "2. æ ¸å¿ƒä¾èµ–æ£€æŸ¥:"
docker exec $CONTAINER_NAME python3 -c "
dependencies = [
    'torch', 'numpy', 'pandas', 'sklearn', 
    'matplotlib', 'seaborn', 'plotly'
]

for dep in dependencies:
    try:
        module = __import__(dep)
        version = getattr(module, '__version__', 'æœªçŸ¥')
        print(f'âœ… {dep}: {version}')
    except ImportError as e:
        print(f'âŒ {dep}: å¯¼å…¥å¤±è´¥')
"

echo "3. æ•°æ®ç›®å½•æ£€æŸ¥:"
docker exec $CONTAINER_NAME bash -c "
echo 'å·¥ä½œç›®å½•å†…å®¹:'
ls -la /workspace/ | head -10
echo ''
echo 'æ•°æ®ç›®å½•å†…å®¹:'
ls -la /app/data/ | head -10
echo ''
echo 'TradeMasterç›®å½•:'
ls -la /home/TradeMaster/ | head -10
"

echo "4. é…ç½®æ–‡ä»¶æ£€æŸ¥:"
docker exec $CONTAINER_NAME bash -c "
echo 'ç¯å¢ƒå˜é‡:'
env | grep -E '(PYTHON|PATH|TRADE)' | sort
echo ''
echo 'æƒé™æ£€æŸ¥:'
whoami
id
ls -la /app/data/ | head -3
"
```

---

## âš¡ æ€§èƒ½é—®é¢˜åˆ†æ

### ğŸ“Š æ€§èƒ½ç›‘æ§è„šæœ¬

#### ç»¼åˆæ€§èƒ½ç›‘æ§
```bash
#!/bin/bash
# performance_monitor.sh

CONTAINER_NAME="trademaster-container"
LOG_FILE="performance_$(date +%Y%m%d_%H%M%S).log"

echo "å¼€å§‹æ€§èƒ½ç›‘æ§ï¼Œæ—¥å¿—æ–‡ä»¶: $LOG_FILE"

{
    echo "=== æ€§èƒ½ç›‘æ§æŠ¥å‘Š $(date) ==="
    
    echo "1. ç³»ç»Ÿè´Ÿè½½:"
    uptime
    
    echo "2. CPUä½¿ç”¨æƒ…å†µ:"
    top -bn1 | head -20
    
    echo "3. å†…å­˜ä½¿ç”¨è¯¦æƒ…:"
    free -h
    docker exec $CONTAINER_NAME cat /proc/meminfo | grep -E "(MemTotal|MemFree|MemAvailable|Cached|Buffers)"
    
    echo "4. ç£ç›˜I/Oç»Ÿè®¡:"
    iostat -x 1 1
    
    echo "5. ç½‘ç»œç»Ÿè®¡:"
    netstat -i
    
    echo "6. å®¹å™¨èµ„æºä½¿ç”¨:"
    docker stats --no-stream $CONTAINER_NAME
    
    echo "7. è¿›ç¨‹æ ‘:"
    docker exec $CONTAINER_NAME pstree -p 1
    
    echo "8. æ–‡ä»¶å¥æŸ„ä½¿ç”¨:"
    docker exec $CONTAINER_NAME bash -c "
        echo 'æ€»é™åˆ¶: '$(ulimit -n)
        echo 'å½“å‰ä½¿ç”¨: '$(lsof | wc -l)
        echo 'æŒ‰è¿›ç¨‹ç»Ÿè®¡:'
        lsof | awk '{print \$2}' | sort | uniq -c | sort -nr | head -5
    "
    
    echo "=== ç›‘æ§å®Œæˆ ==="
    
} | tee -a $LOG_FILE
```

#### Pythonåº”ç”¨æ€§èƒ½åˆ†æ
```python
# performance_profiler.py
import psutil
import time
import threading
import json
from datetime import datetime

class PerformanceProfiler:
    def __init__(self, interval=5):
        self.interval = interval
        self.running = False
        self.data = []
        
    def collect_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        return {
            'timestamp': datetime.now().isoformat(),
            'cpu': {
                'percent': psutil.cpu_percent(interval=1),
                'count': psutil.cpu_count(),
                'freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None
            },
            'memory': {
                'virtual': psutil.virtual_memory()._asdict(),
                'swap': psutil.swap_memory()._asdict()
            },
            'disk': {
                'usage': psutil.disk_usage('/app/data')._asdict(),
                'io': psutil.disk_io_counters()._asdict() if psutil.disk_io_counters() else None
            },
            'network': psutil.net_io_counters()._asdict(),
            'processes': len(psutil.pids()),
            'connections': len(psutil.net_connections())
        }
    
    def monitor_process(self, pid):
        """ç›‘æ§ç‰¹å®šè¿›ç¨‹"""
        try:
            process = psutil.Process(pid)
            return {
                'pid': pid,
                'name': process.name(),
                'cpu_percent': process.cpu_percent(),
                'memory_info': process.memory_info()._asdict(),
                'num_threads': process.num_threads(),
                'open_files': len(process.open_files()),
                'connections': len(process.connections())
            }
        except psutil.NoSuchProcess:
            return None
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.running = True
        
        def monitor_loop():
            while self.running:
                metrics = self.collect_metrics()
                
                # ç›‘æ§Pythonè¿›ç¨‹
                python_processes = []
                for proc in psutil.process_iter(['pid', 'name']):
                    if 'python' in proc.info['name'].lower():
                        proc_metrics = self.monitor_process(proc.info['pid'])
                        if proc_metrics:
                            python_processes.append(proc_metrics)
                
                metrics['python_processes'] = python_processes
                self.data.append(metrics)
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                with open('/app/data/performance_metrics.json', 'w') as f:
                    json.dump(self.data[-100:], f, indent=2)  # åªä¿ç•™æœ€è¿‘100æ¡è®°å½•
                
                time.sleep(self.interval)
        
        self.monitor_thread = threading.Thread(target=monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join()
    
    def get_performance_summary(self):
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.data:
            return "æš‚æ— æ€§èƒ½æ•°æ®"
        
        latest = self.data[-1]
        
        # è®¡ç®—å¹³å‡å€¼ï¼ˆæœ€è¿‘10ä¸ªæ•°æ®ç‚¹ï¼‰
        recent_data = self.data[-10:]
        avg_cpu = sum(d['cpu']['percent'] for d in recent_data) / len(recent_data)
        avg_memory = sum(d['memory']['virtual']['percent'] for d in recent_data) / len(recent_data)
        
        summary = f"""
æ€§èƒ½æ‘˜è¦ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')}):
==========================================
CPUä½¿ç”¨ç‡: {latest['cpu']['percent']:.1f}% (å¹³å‡: {avg_cpu:.1f}%)
å†…å­˜ä½¿ç”¨ç‡: {latest['memory']['virtual']['percent']:.1f}% (å¹³å‡: {avg_memory:.1f}%)
å¯ç”¨å†…å­˜: {latest['memory']['virtual']['available'] / 1024**3:.1f}GB
ç£ç›˜ä½¿ç”¨ç‡: {latest['disk']['usage']['percent']:.1f}%
æ´»è·ƒè¿›ç¨‹æ•°: {latest['processes']}
ç½‘ç»œè¿æ¥æ•°: {latest['connections']}
Pythonè¿›ç¨‹æ•°: {len(latest['python_processes'])}

Pythonè¿›ç¨‹è¯¦æƒ…:
"""
        
        for proc in latest['python_processes']:
            summary += f"  PID {proc['pid']}: CPU {proc['cpu_percent']:.1f}%, å†…å­˜ {proc['memory_info']['rss']/1024**2:.1f}MB\n"
        
        return summary

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    profiler = PerformanceProfiler(interval=10)
    profiler.start_monitoring()
    
    try:
        while True:
            time.sleep(60)
            print(profiler.get_performance_summary())
    except KeyboardInterrupt:
        profiler.stop_monitoring()
        print("æ€§èƒ½ç›‘æ§å·²åœæ­¢")
```

### ğŸš€ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### å†…å­˜ä¼˜åŒ–
```python
# memory_optimizer.py
import gc
import psutil
import torch
import numpy as np

class MemoryOptimizer:
    def __init__(self):
        self.initial_memory = self.get_memory_usage()
        
    def get_memory_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        return {
            'rss': process.memory_info().rss / 1024**2,  # MB
            'vms': process.memory_info().vms / 1024**2,  # MB
            'percent': process.memory_percent()
        }
    
    def optimize_memory(self):
        """æ‰§è¡Œå†…å­˜ä¼˜åŒ–"""
        print("å¼€å§‹å†…å­˜ä¼˜åŒ–...")
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        collected = gc.collect()
        print(f"åƒåœ¾å›æ”¶æ¸…ç†äº† {collected} ä¸ªå¯¹è±¡")
        
        # PyTorchç¼“å­˜æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("æ¸…ç†äº†CUDAç¼“å­˜")
        
        # æ¸…ç†NumPyç¼“å­˜
        if hasattr(np, 'clear_cache'):
            np.clear_cache()
        
        # è·å–ä¼˜åŒ–åçš„å†…å­˜ä½¿ç”¨
        current_memory = self.get_memory_usage()
        saved_memory = self.initial_memory['rss'] - current_memory['rss']
        
        print(f"å†…å­˜ä¼˜åŒ–å®Œæˆï¼ŒèŠ‚çœäº† {saved_memory:.1f}MB")
        return saved_memory
    
    def set_memory_limits(self):
        """è®¾ç½®å†…å­˜ä½¿ç”¨é™åˆ¶"""
        # è®¾ç½®PyTorchå†…å­˜å¢é•¿ç­–ç•¥
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(0.8)
        
        # è®¾ç½®NumPyçº¿ç¨‹æ•°
        import os
        os.environ['OMP_NUM_THREADS'] = '4'
        os.environ['MKL_NUM_THREADS'] = '4'
        
        print("å†…å­˜é™åˆ¶è®¾ç½®å®Œæˆ")
    
    def monitor_memory_usage(self, threshold=80):
        """ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œè¶…è¿‡é˜ˆå€¼æ—¶ä¼˜åŒ–"""
        current = self.get_memory_usage()
        
        if current['percent'] > threshold:
            print(f"å†…å­˜ä½¿ç”¨ç‡ {current['percent']:.1f}% è¶…è¿‡é˜ˆå€¼ {threshold}%ï¼Œå¼€å§‹ä¼˜åŒ–...")
            self.optimize_memory()
            return True
        
        return False

# åœ¨TradeMasterä¸­é›†æˆå†…å­˜ä¼˜åŒ–
def setup_memory_optimization():
    """è®¾ç½®å†…å­˜ä¼˜åŒ–"""
    optimizer = MemoryOptimizer()
    optimizer.set_memory_limits()
    
    # å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨
    import threading
    import time
    
    def memory_monitor():
        while True:
            optimizer.monitor_memory_usage(threshold=80)
            time.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    monitor_thread = threading.Thread(target=memory_monitor, daemon=True)
    monitor_thread.start()
    
    return optimizer
```

#### CPUä¼˜åŒ–é…ç½®
```bash
#!/bin/bash
# cpu_optimization.sh

CONTAINER_NAME="trademaster-container"

echo "å¼€å§‹CPUä¼˜åŒ–é…ç½®..."

# è®¾ç½®CPUäº²å’Œæ€§
docker exec $CONTAINER_NAME bash -c "
# è®¾ç½®Pythonè¿›ç¨‹CPUäº²å’Œæ€§
python_pids=\$(pgrep python)
for pid in \$python_pids; do
    taskset -cp 0-3 \$pid 2>/dev/null || true
done

# è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–å¤šçº¿ç¨‹æ€§èƒ½
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4
export NUMEXPR_NUM_THREADS=4
export OPENBLAS_NUM_THREADS=4

# ä¼˜åŒ–ç³»ç»Ÿè°ƒåº¦
echo 'performance' > /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor 2>/dev/null || true
"

# æ›´æ–°å®¹å™¨CPUé…ç½®
docker update --cpus="4.0" --cpu-shares=1024 $CONTAINER_NAME

echo "CPUä¼˜åŒ–é…ç½®å®Œæˆ"
```

---

## ğŸ› å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ

### ğŸ”´ å¯åŠ¨é”™è¯¯

#### é”™è¯¯1: å®¹å™¨å¯åŠ¨åç«‹å³é€€å‡º

**é”™è¯¯ä¿¡æ¯**:
```
trademaster-container exited with code 0
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs trademaster-container

# æ£€æŸ¥å¯åŠ¨å‘½ä»¤
docker inspect trademaster-container | jq '.[0].Config.Cmd'

# æ‰‹åŠ¨è¿è¡Œè°ƒè¯•
docker run -it --rm trademaster:latest /bin/bash
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: ä¿®æ”¹å¯åŠ¨å‘½ä»¤ï¼Œä¿æŒå®¹å™¨è¿è¡Œ
docker run -d \
  --name trademaster-container \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest tail -f /dev/null

# æ–¹æ¡ˆ2: ä½¿ç”¨æ­£ç¡®çš„å…¥å£ç‚¹
docker run -d \
  --name trademaster-container \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest /entrypoint.sh bash

# æ–¹æ¡ˆ3: äº¤äº’å¼å¯åŠ¨è°ƒè¯•
docker run -it \
  --name trademaster-container \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest /bin/bash
```

#### é”™è¯¯2: ç«¯å£ç»‘å®šå¤±è´¥

**é”™è¯¯ä¿¡æ¯**:
```
Error starting userland proxy: listen tcp 0.0.0.0:8080: bind: address already in use
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tuln | grep 8080
ss -tuln | grep 8080

# Windowsç³»ç»Ÿ
netstat -ano | findstr :8080

# æŸ¥æ‰¾å ç”¨è¿›ç¨‹
lsof -i :8080  # Linux/Mac
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: åœæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹
kill -9 $(lsof -ti:8080)  # Linux/Mac
taskkill /PID <PID> /F    # Windows

# æ–¹æ¡ˆ2: ä½¿ç”¨ä¸åŒç«¯å£
docker run -d \
  --name trademaster-container \
  -p 8081:8080 \  # ä¿®æ”¹æœ¬åœ°ç«¯å£
  -p 8889:8888 \
  -p 5002:5000 \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest

# æ–¹æ¡ˆ3: ä½¿ç”¨ä¸»æœºç½‘ç»œæ¨¡å¼
docker run -d \
  --name trademaster-container \
  --network host \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest
```

### ğŸŸ¡ è¿è¡Œæ—¶é”™è¯¯

#### é”™è¯¯3: æ¨¡å—å¯¼å…¥å¤±è´¥

**é”™è¯¯ä¿¡æ¯**:
```python
ModuleNotFoundError: No module named 'trademaster'
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥Pythonè·¯å¾„
docker exec trademaster-container python3 -c "
import sys
print('Pythonç‰ˆæœ¬:', sys.version)
print('Pythonè·¯å¾„:')
for path in sys.path:
    print('  ', path)
"

# æ£€æŸ¥TradeMasterç›®å½•
docker exec trademaster-container ls -la /home/TradeMaster/

# æ£€æŸ¥ç¯å¢ƒå˜é‡
docker exec trademaster-container env | grep PYTHON
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: è®¾ç½®PYTHONPATHç¯å¢ƒå˜é‡
docker exec trademaster-container bash -c "
export PYTHONPATH='/home/TradeMaster:\$PYTHONPATH'
python3 -c 'import trademaster; print(\"æˆåŠŸ\")'
"

# æ–¹æ¡ˆ2: åœ¨å®¹å™¨å¯åŠ¨æ—¶è®¾ç½®ç¯å¢ƒå˜é‡
docker run -d \
  --name trademaster-container \
  -e PYTHONPATH="/home/TradeMaster" \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest

# æ–¹æ¡ˆ3: åˆ›å»ºç¬¦å·é“¾æ¥
docker exec trademaster-container bash -c "
ln -sf /home/TradeMaster/trademaster /opt/trademaster-env/lib/python3.8/site-packages/
"
```

#### é”™è¯¯4: æƒé™è¢«æ‹’ç»

**é”™è¯¯ä¿¡æ¯**:
```
PermissionError: [Errno 13] Permission denied: '/app/data/model.pkl'
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥æ–‡ä»¶æƒé™
docker exec trademaster-container ls -la /app/data/

# æ£€æŸ¥ç”¨æˆ·èº«ä»½
docker exec trademaster-container whoami
docker exec trademaster-container id

# æ£€æŸ¥æŒ‚è½½ç‚¹æƒé™
ls -la ./data/
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: ä¿®æ”¹æœ¬åœ°æ–‡ä»¶æƒé™
sudo chown -R $(whoami):$(whoami) ./data/
chmod -R 755 ./data/

# æ–¹æ¡ˆ2: åœ¨å®¹å™¨å†…ä¿®æ”¹æƒé™
docker exec trademaster-container bash -c "
chown -R \$(whoami):\$(whoami) /app/data/
chmod -R 755 /app/data/
"

# æ–¹æ¡ˆ3: ä½¿ç”¨æ­£ç¡®çš„ç”¨æˆ·æƒé™å¯åŠ¨å®¹å™¨
docker run -d \
  --name trademaster-container \
  --user $(id -u):$(id -g) \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest
```

#### é”™è¯¯5: å†…å­˜ä¸è¶³

**é”™è¯¯ä¿¡æ¯**:
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥å®¹å™¨å†…å­˜é™åˆ¶
docker stats trademaster-container

# æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨
docker exec trademaster-container nvidia-smi

# æ£€æŸ¥ç³»ç»Ÿå†…å­˜
free -h
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: å¢åŠ å®¹å™¨å†…å­˜é™åˆ¶
docker update --memory="16g" --memory-swap="20g" trademaster-container
docker restart trademaster-container

# æ–¹æ¡ˆ2: åœ¨Pythonä¸­è®¾ç½®å†…å­˜ç®¡ç†
docker exec trademaster-container python3 -c "
import torch
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.7)
    torch.cuda.empty_cache()
    print('GPUå†…å­˜è®¾ç½®å®Œæˆ')
"

# æ–¹æ¡ˆ3: ä½¿ç”¨å†…å­˜æ˜ å°„æ¨¡å¼
docker exec trademaster-container python3 -c "
import torch
# å¯ç”¨å†…å­˜æ˜ å°„
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.enabled = True
"
```

### ğŸ”µ ç½‘ç»œé—®é¢˜

#### é”™è¯¯6: ç½‘ç»œè¿æ¥å¤±è´¥

**é”™è¯¯ä¿¡æ¯**:
```
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.example.com', port=443)
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥å®¹å™¨ç½‘ç»œé…ç½®
docker inspect trademaster-container | jq '.[0].NetworkSettings'

# æµ‹è¯•ç½‘ç»œè¿é€šæ€§
docker exec trademaster-container ping google.com
docker exec trademaster-container nslookup google.com

# æ£€æŸ¥DNSé…ç½®
docker exec trademaster-container cat /etc/resolv.conf
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ1: è®¾ç½®DNSæœåŠ¡å™¨
docker run -d \
  --name trademaster-container \
  --dns 8.8.8.8 \
  --dns 114.114.114.114 \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest

# æ–¹æ¡ˆ2: ä½¿ç”¨ä¸»æœºç½‘ç»œ
docker run -d \
  --name trademaster-container \
  --network host \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest

# æ–¹æ¡ˆ3: é…ç½®ä»£ç†
docker run -d \
  --name trademaster-container \
  -e HTTP_PROXY=http://proxy.company.com:8080 \
  -e HTTPS_PROXY=http://proxy.company.com:8080 \
  # ... å…¶ä»–å‚æ•°
  trademaster:latest
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—åˆ†æ

### ğŸ“ æ—¥å¿—æ”¶é›†ä¸åˆ†æ

#### é›†ä¸­æ—¥å¿—æ”¶é›†
```bash
#!/bin/bash
# log_collector.sh

LOG_DIR="/opt/trademaster/logs"
DATE=$(date +%Y%m%d_%H%M%S)

mkdir -p $LOG_DIR

echo "å¼€å§‹æ”¶é›†æ—¥å¿— - $DATE"

# æ”¶é›†å®¹å™¨æ—¥å¿—
docker logs trademaster-container > "$LOG_DIR/container_$DATE.log" 2>&1

# æ”¶é›†ç³»ç»Ÿæ—¥å¿—
journalctl -u docker.service --since "1 hour ago" > "$LOG_DIR/docker_system_$DATE.log"

# æ”¶é›†åº”ç”¨æ—¥å¿—
docker exec trademaster-container bash -c "
find /app/logs -name '*.log' -mtime -1 -exec cp {} /tmp/ \;
tar czf /tmp/app_logs_$DATE.tar.gz /tmp/*.log 2>/dev/null || true
" 

docker cp trademaster-container:/tmp/app_logs_$DATE.tar.gz "$LOG_DIR/"

# æ”¶é›†æ€§èƒ½æ•°æ®
docker exec trademaster-container bash -c "
if [ -f /app/data/performance_metrics.json ]; then
    cp /app/data/performance_metrics.json /tmp/performance_$DATE.json
fi
"

docker cp trademaster-container:/tmp/performance_$DATE.json "$LOG_DIR/" 2>/dev/null || true

echo "æ—¥å¿—æ”¶é›†å®Œæˆï¼Œå­˜å‚¨ä½ç½®: $LOG_DIR"
```

#### æ—¥å¿—åˆ†æè„šæœ¬
```python
# log_analyzer.py
import re
import json
from datetime import datetime, timedelta
from collections import defaultdict, Counter

class LogAnalyzer:
    def __init__(self, log_file):
        self.log_file = log_file
        self.error_patterns = [
            r'ERROR:.*',
            r'Exception:.*',
            r'Traceback.*',
            r'FATAL:.*',
            r'CRITICAL:.*'
        ]
        
    def parse_container_logs(self):
        """è§£æå®¹å™¨æ—¥å¿—"""
        errors = []
        warnings = []
        info_messages = []
        
        with open(self.log_file, 'r') as f:
            for line_no, line in enumerate(f, 1):
                line = line.strip()
                
                # æå–æ—¶é—´æˆ³
                timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2})', line)
                timestamp = timestamp_match.group(1) if timestamp_match else None
                
                # åˆ†ç±»æ—¥å¿—çº§åˆ«
                if any(re.search(pattern, line, re.IGNORECASE) for pattern in self.error_patterns):
                    errors.append({
                        'line_no': line_no,
                        'timestamp': timestamp,
                        'message': line
                    })
                elif 'WARNING' in line.upper() or 'WARN' in line.upper():
                    warnings.append({
                        'line_no': line_no,
                        'timestamp': timestamp,
                        'message': line
                    })
                elif 'INFO' in line.upper():
                    info_messages.append({
                        'line_no': line_no,
                        'timestamp': timestamp,
                        'message': line
                    })
        
        return {
            'errors': errors,
            'warnings': warnings,
            'info': info_messages
        }
    
    def analyze_error_patterns(self, logs):
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        error_types = Counter()
        error_modules = Counter()
        
        for error in logs['errors']:
            message = error['message']
            
            # æå–é”™è¯¯ç±»å‹
            error_type_match = re.search(r'(\w+Error|\w+Exception)', message)
            if error_type_match:
                error_types[error_type_match.group(1)] += 1
            
            # æå–æ¨¡å—å
            module_match = re.search(r'File ".*?/([^/]+\.py)"', message)
            if module_match:
                error_modules[module_match.group(1)] += 1
        
        return {
            'error_types': dict(error_types.most_common(10)),
            'error_modules': dict(error_modules.most_common(10))
        }
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        logs = self.parse_container_logs()
        patterns = self.analyze_error_patterns(logs)
        
        report = f"""
æ—¥å¿—åˆ†ææŠ¥å‘Š
=============
æ–‡ä»¶: {self.log_file}
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ç»Ÿè®¡ä¿¡æ¯:
- é”™è¯¯æ•°é‡: {len(logs['errors'])}
- è­¦å‘Šæ•°é‡: {len(logs['warnings'])}
- ä¿¡æ¯æ•°é‡: {len(logs['info'])}

æœ€å¸¸è§é”™è¯¯ç±»å‹:
"""
        
        for error_type, count in patterns['error_types'].items():
            report += f"- {error_type}: {count}æ¬¡\n"
        
        report += "\næœ€å¸¸å‡ºé”™æ¨¡å—:\n"
        for module, count in patterns['error_modules'].items():
            report += f"- {module}: {count}æ¬¡\n"
        
        report += "\næœ€è¿‘10ä¸ªé”™è¯¯:\n"
        for error in logs['errors'][-10:]:
            report += f"[è¡Œ{error['line_no']}] {error['timestamp']}: {error['message'][:100]}...\n"
        
        return report
    
    def export_json(self, output_file):
        """å¯¼å‡ºJSONæ ¼å¼çš„åˆ†æç»“æœ"""
        logs = self.parse_container_logs()
        patterns = self.analyze_error_patterns(logs)
        
        result = {
            'analysis_time': datetime.now().isoformat(),
            'log_file': self.log_file,
            'summary': {
                'error_count': len(logs['errors']),
                'warning_count': len(logs['warnings']),
                'info_count': len(logs['info'])
            },
            'patterns': patterns,
            'recent_errors': logs['errors'][-20:]  # æœ€è¿‘20ä¸ªé”™è¯¯
        }
        
        with open(output_file, 'w') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    analyzer = LogAnalyzer('/opt/trademaster/logs/container_latest.log')
    report = analyzer.generate_report()
    print(report)
    
    # å¯¼å‡ºè¯¦ç»†åˆ†æ
    analyzer.export_json('/opt/trademaster/logs/analysis_report.json')
```

### ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æ

#### æ€§èƒ½æ•°æ®åˆ†æ
```python
# performance_analyzer.py
import json
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import numpy as np

class PerformanceAnalyzer:
    def __init__(self, metrics_file):
        self.metrics_file = metrics_file
        self.data = self.load_data()
        
    def load_data(self):
        """åŠ è½½æ€§èƒ½æ•°æ®"""
        try:
            with open(self.metrics_file, 'r') as f:
                data = json.load(f)
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.json_normalize(data)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            return df.sort_values('timestamp')
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def analyze_cpu_usage(self):
        """åˆ†æCPUä½¿ç”¨æƒ…å†µ"""
        if self.data.empty:
            return None
            
        cpu_stats = {
            'mean': self.data['cpu.percent'].mean(),
            'max': self.data['cpu.percent'].max(),
            'min': self.data['cpu.percent'].min(),
            'std': self.data['cpu.percent'].std(),
            'p95': self.data['cpu.percent'].quantile(0.95),
            'p99': self.data['cpu.percent'].quantile(0.99)
        }
        
        # æ£€æµ‹CPUå¼‚å¸¸
        threshold = cpu_stats['mean'] + 2 * cpu_stats['std']
        high_cpu_periods = self.data[self.data['cpu.percent'] > threshold]
        
        return {
            'statistics': cpu_stats,
            'high_usage_periods': len(high_cpu_periods),
            'recommendations': self.get_cpu_recommendations(cpu_stats)
        }
    
    def analyze_memory_usage(self):
        """åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.data.empty:
            return None
            
        memory_stats = {
            'mean_percent': self.data['memory.virtual.percent'].mean(),
            'max_percent': self.data['memory.virtual.percent'].max(),
            'mean_available_gb': self.data['memory.virtual.available'].mean() / 1024**3,
            'min_available_gb': self.data['memory.virtual.available'].min() / 1024**3
        }
        
        # å†…å­˜æ³„æ¼æ£€æµ‹
        if len(self.data) > 10:
            recent_trend = self.data['memory.virtual.percent'].tail(10).mean()
            earlier_trend = self.data['memory.virtual.percent'].head(10).mean()
            memory_growth = recent_trend - earlier_trend
        else:
            memory_growth = 0
        
        return {
            'statistics': memory_stats,
            'memory_growth': memory_growth,
            'recommendations': self.get_memory_recommendations(memory_stats, memory_growth)
        }
    
    def analyze_disk_usage(self):
        """åˆ†æç£ç›˜ä½¿ç”¨æƒ…å†µ"""
        if self.data.empty:
            return None
            
        disk_stats = {
            'usage_percent': self.data['disk.usage.percent'].iloc[-1],
            'free_gb': self.data['disk.usage.free'].iloc[-1] / 1024**3,
            'total_gb': self.data['disk.usage.total'].iloc[-1] / 1024**3
        }
        
        return {
            'statistics': disk_stats,
            'recommendations': self.get_disk_recommendations(disk_stats)
        }
    
    def get_cpu_recommendations(self, stats):
        """CPUä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if stats['mean'] > 80:
            recommendations.append("CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ CPUæ ¸å¿ƒæ•°æˆ–ä¼˜åŒ–ç®—æ³•")
        elif stats['mean'] < 20:
            recommendations.append("CPUä½¿ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥è€ƒè™‘å‡å°‘åˆ†é…çš„CPUèµ„æº")
        
        if stats['max'] > 95:
            recommendations.append("æ£€æµ‹åˆ°CPUå³°å€¼è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨è®¡ç®—å¯†é›†å‹ä»»åŠ¡")
        
        if stats['std'] > 30:
            recommendations.append("CPUä½¿ç”¨ç‡æ³¢åŠ¨è¾ƒå¤§ï¼Œå»ºè®®ä¼˜åŒ–ä»»åŠ¡è°ƒåº¦")
        
        return recommendations
    
    def get_memory_recommendations(self, stats, growth):
        """å†…å­˜ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if stats['mean_percent'] > 85:
            recommendations.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        
        if stats['min_available_gb'] < 1:
            recommendations.append("å¯ç”¨å†…å­˜è¿‡ä½ï¼Œå­˜åœ¨OOMé£é™©")
        
        if growth > 10:
            recommendations.append("æ£€æµ‹åˆ°å†…å­˜å¢é•¿è¶‹åŠ¿ï¼Œå¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")
        
        return recommendations
    
    def get_disk_recommendations(self, stats):
        """ç£ç›˜ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if stats['usage_percent'] > 85:
            recommendations.append("ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®æ¸…ç†æ•°æ®æˆ–æ‰©å®¹")
        
        if stats['free_gb'] < 5:
            recommendations.append("ç£ç›˜å‰©ä½™ç©ºé—´ä¸è¶³ï¼Œéœ€è¦ç«‹å³å¤„ç†")
        
        return recommendations
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        cpu_analysis = self.analyze_cpu_usage()
        memory_analysis = self.analyze_memory_usage()
        disk_analysis = self.analyze_disk_usage()
        
        report = f"""
æ€§èƒ½åˆ†ææŠ¥å‘Š
============
åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ•°æ®ç‚¹æ•°é‡: {len(self.data)}
åˆ†ææ—¶é—´èŒƒå›´: {self.data['timestamp'].min()} åˆ° {self.data['timestamp'].max()}

CPUåˆ†æ:
"""
        if cpu_analysis:
            report += f"""
- å¹³å‡ä½¿ç”¨ç‡: {cpu_analysis['statistics']['mean']:.1f}%
- æœ€é«˜ä½¿ç”¨ç‡: {cpu_analysis['statistics']['max']:.1f}%
- P95ä½¿ç”¨ç‡: {cpu_analysis['statistics']['p95']:.1f}%
- é«˜è´Ÿè½½æ—¶æ®µ: {cpu_analysis['high_usage_periods']}æ¬¡

CPUä¼˜åŒ–å»ºè®®:
"""
            for rec in cpu_analysis['recommendations']:
                report += f"- {rec}\n"
        
        report += "\nå†…å­˜åˆ†æ:\n"
        if memory_analysis:
            report += f"""
- å¹³å‡ä½¿ç”¨ç‡: {memory_analysis['statistics']['mean_percent']:.1f}%
- æœ€é«˜ä½¿ç”¨ç‡: {memory_analysis['statistics']['max_percent']:.1f}%
- å¹³å‡å¯ç”¨å†…å­˜: {memory_analysis['statistics']['mean_available_gb']:.1f}GB
- å†…å­˜å¢é•¿è¶‹åŠ¿: {memory_analysis['memory_growth']:.1f}%

å†…å­˜ä¼˜åŒ–å»ºè®®:
"""
            for rec in memory_analysis['recommendations']:
                report += f"- {rec}\n"
        
        report += "\nç£ç›˜åˆ†æ:\n"
        if disk_analysis:
            report += f"""
- å½“å‰ä½¿ç”¨ç‡: {disk_analysis['statistics']['usage_percent']:.1f}%
- å‰©ä½™ç©ºé—´: {disk_analysis['statistics']['free_gb']:.1f}GB
- æ€»å®¹é‡: {disk_analysis['statistics']['total_gb']:.1f}GB

ç£ç›˜ä¼˜åŒ–å»ºè®®:
"""
            for rec in disk_analysis['recommendations']:
                report += f"- {rec}\n"
        
        return report
    
    def plot_performance_trends(self, output_dir='/app/data'):
        """ç»˜åˆ¶æ€§èƒ½è¶‹åŠ¿å›¾"""
        if self.data.empty:
            print("æ— æ•°æ®å¯ç»˜åˆ¶")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('TradeMaster æ€§èƒ½è¶‹åŠ¿åˆ†æ', fontsize=16)
        
        # CPUä½¿ç”¨ç‡è¶‹åŠ¿
        axes[0, 0].plot(self.data['timestamp'], self.data['cpu.percent'])
        axes[0, 0].set_title('CPUä½¿ç”¨ç‡è¶‹åŠ¿')
        axes[0, 0].set_ylabel('CPU %')
        axes[0, 0].grid(True)
        
        # å†…å­˜ä½¿ç”¨ç‡è¶‹åŠ¿
        axes[0, 1].plot(self.data['timestamp'], self.data['memory.virtual.percent'])
        axes[0, 1].set_title('å†…å­˜ä½¿ç”¨ç‡è¶‹åŠ¿')
        axes[0, 1].set_ylabel('å†…å­˜ %')
        axes[0, 1].grid(True)
        
        # ç£ç›˜ä½¿ç”¨ç‡è¶‹åŠ¿
        axes[1, 0].plot(self.data['timestamp'], self.data['disk.usage.percent'])
        axes[1, 0].set_title('ç£ç›˜ä½¿ç”¨ç‡è¶‹åŠ¿')
        axes[1, 0].set_ylabel('ç£ç›˜ %')
        axes[1, 0].grid(True)
        
        # è¿›ç¨‹æ•°é‡è¶‹åŠ¿
        axes[1, 1].plot(self.data['timestamp'], self.data['processes'])
        axes[1, 1].set_title('è¿›ç¨‹æ•°é‡è¶‹åŠ¿')
        axes[1, 1].set_ylabel('è¿›ç¨‹æ•°')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/performance_trends.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"æ€§èƒ½è¶‹åŠ¿å›¾å·²ä¿å­˜åˆ°: {output_dir}/performance_trends.png")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    analyzer = PerformanceAnalyzer('/app/data/performance_metrics.json')
    report = analyzer.generate_performance_report()
    print(report)
    analyzer.plot_performance_trends()
```

---

## ğŸ› ï¸ è°ƒè¯•å·¥å…·ç®±

### ğŸ” äº¤äº’å¼è°ƒè¯•ç¯å¢ƒ

#### è¿œç¨‹è°ƒè¯•é…ç½®
```python
# remote_debugger.py
import pdb
import sys
import threading
import socket
from contextlib import contextmanager

class RemoteDebugger:
    def __init__(self, host='0.0.0.0', port=5678):
        self.host = host
        self.port = port
        self.debugger_active = False
        
    def start_remote_debugger(self):
        """å¯åŠ¨è¿œç¨‹è°ƒè¯•æœåŠ¡"""
        try:
            import ptvsd
            ptvsd.enable_attach(address=(self.host, self.port), redirect_output=True)
            print(f"è¿œç¨‹è°ƒè¯•å™¨å·²å¯åŠ¨ï¼Œè¿æ¥åœ°å€: {self.host}:{self.port}")
            self.debugger_active = True
        except ImportError:
            print("ptvsdæœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡†pdbè°ƒè¯•å™¨")
            self.start_pdb_server()
    
    def start_pdb_server(self):
        """å¯åŠ¨PDBæœåŠ¡å™¨"""
        def pdb_server():
            server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            server.bind((self.host, self.port))
            server.listen(1)
            
            print(f"PDBè°ƒè¯•æœåŠ¡å™¨å¯åŠ¨: {self.host}:{self.port}")
            
            while True:
                try:
                    client, addr = server.accept()
                    print(f"è°ƒè¯•å®¢æˆ·ç«¯è¿æ¥: {addr}")
                    
                    # é‡å®šå‘stdin/stdoutåˆ°å®¢æˆ·ç«¯
                    sys.stdin = client.makefile('r')
                    sys.stdout = client.makefile('w')
                    
                    pdb.set_trace()
                    
                except Exception as e:
                    print(f"è°ƒè¯•æœåŠ¡å™¨é”™è¯¯: {e}")
                finally:
                    client.close()
        
        debug_thread = threading.Thread(target=pdb_server, daemon=True)
        debug_thread.start()
        self.debugger_active = True
    
    @contextmanager
    def debug_context(self, condition=True):
        """è°ƒè¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if condition and self.debugger_active:
            try:
                import ptvsd
                ptvsd.break_into_debugger()
            except ImportError:
                pdb.set_trace()
        yield
    
    def log_debug_info(self, obj, name="debug_object"):
        """è®°å½•è°ƒè¯•ä¿¡æ¯"""
        debug_info = {
            'name': name,
            'type': type(obj).__name__,
            'value': str(obj)[:1000],  # é™åˆ¶é•¿åº¦
            'attributes': [attr for attr in dir(obj) if not attr.startswith('_')]
        }
        
        with open('/app/data/debug_log.json', 'a') as f:
            import json
            f.write(json.dumps(debug_info) + '\n')
        
        print(f"è°ƒè¯•ä¿¡æ¯å·²è®°å½•: {name}")

# åœ¨TradeMasterä¸­é›†æˆè°ƒè¯•å™¨
def setup_debugging():
    """è®¾ç½®è°ƒè¯•ç¯å¢ƒ"""
    debugger = RemoteDebugger()
    
    # åœ¨å¼€å‘ç¯å¢ƒä¸­å¯åŠ¨è¿œç¨‹è°ƒè¯•
    import os
    if os.getenv('DEBUG', '').lower() == 'true':
        debugger.start_remote_debugger()
    
    return debugger
```

#### æ€§èƒ½åˆ†æå·¥å…·
```python
# profiling_tools.py
import cProfile
import pstats
import functools
import time
import traceback
from contextlib import contextmanager
import psutil
import threading

class ProfilerManager:
    def __init__(self):
        self.profiles = {}
        self.active_profilers = {}
        
    def profile_function(self, func_name=None):
        """å‡½æ•°æ€§èƒ½åˆ†æè£…é¥°å™¨"""
        def decorator(func):
            name = func_name or func.__name__
            
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                profiler = cProfile.Profile()
                profiler.enable()
                
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    profiler.disable()
                    end_time = time.time()
                    
                    # ä¿å­˜æ€§èƒ½æ•°æ®
                    stats = pstats.Stats(profiler)
                    self.profiles[name] = {
                        'stats': stats,
                        'execution_time': end_time - start_time,
                        'timestamp': time.time()
                    }
                    
                    # è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
                    self.save_profile_report(name)
            
            return wrapper
        return decorator
    
    @contextmanager
    def profile_context(self, name="context_profile"):
        """ä¸Šä¸‹æ–‡æ€§èƒ½åˆ†æ"""
        profiler = cProfile.Profile()
        profiler.enable()
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        
        try:
            yield
        finally:
            profiler.disable()
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            stats = pstats.Stats(profiler)
            self.profiles[name] = {
                'stats': stats,
                'execution_time': end_time - start_time,
                'memory_delta': end_memory - start_memory,
                'timestamp': time.time()
            }
            
            self.save_profile_report(name)
    
    def save_profile_report(self, name):
        """ä¿å­˜æ€§èƒ½æŠ¥å‘Š"""
        if name not in self.profiles:
            return
        
        profile_data = self.profiles[name]
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        with open(f'/app/data/profile_{name}.txt', 'w') as f:
            f.write(f"æ€§èƒ½åˆ†ææŠ¥å‘Š: {name}\n")
            f.write(f"æ‰§è¡Œæ—¶é—´: {profile_data['execution_time']:.4f}ç§’\n")
            if 'memory_delta' in profile_data:
                f.write(f"å†…å­˜å˜åŒ–: {profile_data['memory_delta'] / 1024**2:.2f}MB\n")
            f.write(f"åˆ†ææ—¶é—´: {time.ctime(profile_data['timestamp'])}\n")
            f.write("\n" + "="*50 + "\n")
            
            # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡
            stats = profile_data['stats']
            stats.sort_stats('cumulative')
            stats.print_stats(20, file=f)
        
        print(f"æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: /app/data/profile_{name}.txt")
    
    def start_continuous_profiling(self, duration=300):
        """å¯åŠ¨æŒç»­æ€§èƒ½åˆ†æ"""
        def continuous_profile():
            profiler = cProfile.Profile()
            profiler.enable()
            
            start_time = time.time()
            
            # ç­‰å¾…æŒ‡å®šæ—¶é—´
            time.sleep(duration)
            
            profiler.disable()
            end_time = time.time()
            
            # ä¿å­˜ç»“æœ
            stats = pstats.Stats(profiler)
            self.profiles['continuous'] = {
                'stats': stats,
                'execution_time': end_time - start_time,
                'timestamp': time.time()
            }
            
            self.save_profile_report('continuous')
            print(f"æŒç»­æ€§èƒ½åˆ†æå®Œæˆï¼Œæ—¶é•¿: {duration}ç§’")
        
        profile_thread = threading.Thread(target=continuous_profile, daemon=True)
        profile_thread.start()
        
        print(f"å¼€å§‹æŒç»­æ€§èƒ½åˆ†æï¼Œæ—¶é•¿: {duration}ç§’")
    
    def memory_profiler(self, func):
        """å†…å­˜ä½¿ç”¨åˆ†æè£…é¥°å™¨"""
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            import tracemalloc
            
            # å¼€å§‹å†…å­˜è·Ÿè¸ª
            tracemalloc.start()
            start_memory = psutil.Process().memory_info().rss
            
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                # è·å–å†…å­˜å¿«ç…§
                current, peak = tracemalloc.get_traced_memory()
                tracemalloc.stop()
                
                end_memory = psutil.Process().memory_info().rss
                
                # è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
                memory_report = {
                    'function': func.__name__,
                    'current_traced': current / 1024**2,  # MB
                    'peak_traced': peak / 1024**2,        # MB
                    'rss_delta': (end_memory - start_memory) / 1024**2,  # MB
                    'timestamp': time.time()
                }
                
                with open('/app/data/memory_profile.json', 'a') as f:
                    import json
                    f.write(json.dumps(memory_report) + '\n')
                
                print(f"å†…å­˜åˆ†æå®Œæˆ: {func.__name__}")
                print(f"  å½“å‰å†…å­˜: {current / 1024**2:.2f}MB")
                print(f"  å³°å€¼å†…å­˜: {peak / 1024**2:.2f}MB")
                print(f"  RSSå˜åŒ–: {(end_memory - start_memory) / 1024**2:.2f}MB")
        
        return wrapper

# ä½¿ç”¨ç¤ºä¾‹
profiler_manager = ProfilerManager()

# è£…é¥°å™¨ä½¿ç”¨
@profiler_manager.profile_function("model_training")
def train_model():
    # æ¨¡å‹è®­ç»ƒä»£ç 
    pass

@profiler_manager.memory_profiler
def load_large_dataset():
    # æ•°æ®åŠ è½½ä»£ç 
    pass

# ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½¿ç”¨
def some_function():
    with profiler_manager.profile_context("data_processing"):
        # æ•°æ®å¤„ç†ä»£ç 
        pass
```

### ğŸ§ª æµ‹è¯•å’ŒéªŒè¯å·¥å…·

#### é›†æˆæµ‹è¯•å¥—ä»¶
```bash
#!/bin/bash
# integration_test.sh

CONTAINER_NAME="trademaster-container"
TEST_RESULTS_DIR="/tmp/test_results"

mkdir -p $TEST_RESULTS_DIR

echo "å¼€å§‹é›†æˆæµ‹è¯•..."

# æµ‹è¯•1: åŸºç¡€ç¯å¢ƒæµ‹è¯•
echo "1. åŸºç¡€ç¯å¢ƒæµ‹è¯•..."
docker exec $CONTAINER_NAME python3 -c "
import sys
import os
import subprocess

print('Pythonç‰ˆæœ¬:', sys.version)
print('å½“å‰ç”¨æˆ·:', os.getenv('USER', 'unknown'))
print('å·¥ä½œç›®å½•:', os.getcwd())
print('PYTHONPATH:', sys.path[:3])

# æµ‹è¯•åŸºç¡€å‘½ä»¤
try:
    result = subprocess.run(['which', 'python3'], capture_output=True, text=True)
    print('Pythonè·¯å¾„:', result.stdout.strip())
except Exception as e:
    print('å‘½ä»¤æµ‹è¯•å¤±è´¥:', e)
" > "$TEST_RESULTS_DIR/basic_env_test.txt"

# æµ‹è¯•2: æ¨¡å—å¯¼å…¥æµ‹è¯•
echo "2. æ¨¡å—å¯¼å…¥æµ‹è¯•..."
docker exec $CONTAINER_NAME python3 -c "
import importlib
import traceback

modules_to_test = [
    'trademaster',
    'torch',
    'numpy',
    'pandas',
    'sklearn',
    'matplotlib',
    'plotly',
    'yfinance'
]

results = {}
for module in modules_to_test:
    try:
        imported_module = importlib.import_module(module)
        version = getattr(imported_module, '__version__', 'unknown')
        results[module] = {'status': 'success', 'version': version}
        print(f'âœ… {module}: {version}')
    except Exception as e:
        results[module] = {'status': 'failed', 'error': str(e)}
        print(f'âŒ {module}: {str(e)}')

print('\næ¨¡å—å¯¼å…¥æµ‹è¯•å®Œæˆ')
print('æˆåŠŸ:', sum(1 for r in results.values() if r['status'] == 'success'))
print('å¤±è´¥:', sum(1 for r in results.values() if r['status'] == 'failed'))
" > "$TEST_RESULTS_DIR/module_import_test.txt"

# æµ‹è¯•3: æ•°æ®è®¿é—®æµ‹è¯•
echo "3. æ•°æ®è®¿é—®æµ‹è¯•..."
docker exec $CONTAINER_NAME bash -c "
echo 'æµ‹è¯•æ•°æ®ç›®å½•è®¿é—®...'
echo 'å·¥ä½œç›®å½•å†…å®¹:'
ls -la /workspace/ | head -5
echo ''
echo 'æ•°æ®ç›®å½•å†…å®¹:'
ls -la /app/data/ | head -5
echo ''
echo 'TradeMasterç›®å½•å†…å®¹:'
ls -la /home/TradeMaster/ | head -5

echo ''
echo 'æƒé™æµ‹è¯•:'
echo 'åˆ›å»ºæµ‹è¯•æ–‡ä»¶...'
echo 'test data' > /app/data/test_file.txt
if [ -f /app/data/test_file.txt ]; then
    echo 'âœ… æ•°æ®ç›®å½•å†™å…¥æƒé™æ­£å¸¸'
    rm /app/data/test_file.txt
else
    echo 'âŒ æ•°æ®ç›®å½•å†™å…¥æƒé™å¼‚å¸¸'
fi
" > "$TEST_RESULTS_DIR/data_access_test.txt"

# æµ‹è¯•4: ç½‘ç»œè¿æ¥æµ‹è¯•
echo "4. ç½‘ç»œè¿æ¥æµ‹è¯•..."
docker exec $CONTAINER_NAME bash -c "
echo 'ç½‘ç»œè¿é€šæ€§æµ‹è¯•...'

# æµ‹è¯•DNSè§£æ
if nslookup google.com >/dev/null 2>&1; then
    echo 'âœ… DNSè§£ææ­£å¸¸'
else
    echo 'âŒ DNSè§£æå¤±è´¥'
fi

# æµ‹è¯•HTTPè¿æ¥
if curl -s --connect-timeout 10 http://www.google.com >/dev/null; then
    echo 'âœ… HTTPè¿æ¥æ­£å¸¸'
else
    echo 'âŒ HTTPè¿æ¥å¤±è´¥'
fi

# æµ‹è¯•HTTPSè¿æ¥
if curl -s --connect-timeout 10 https://pypi.org >/dev/null; then
    echo 'âœ… HTTPSè¿æ¥æ­£å¸¸'
else
    echo 'âŒ HTTPSè¿æ¥å¤±è´¥'
fi

echo ''
echo 'ç«¯å£ç›‘å¬æµ‹è¯•:'
netstat -tuln | grep -E ':(8080|8888|5000)' || echo 'ç«¯å£æœªç›‘å¬'
" > "$TEST_RESULTS_DIR/network_test.txt"

# æµ‹è¯•5: æ€§èƒ½åŸºå‡†æµ‹è¯•
echo "5. æ€§èƒ½åŸºå‡†æµ‹è¯•..."
docker exec $CONTAINER_NAME python3 -c "
import time
import numpy as np
import torch

print('å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...')

# CPUè®¡ç®—æ€§èƒ½æµ‹è¯•
print('1. CPUè®¡ç®—æ€§èƒ½æµ‹è¯•:')
start_time = time.time()
a = np.random.rand(1000, 1000)
b = np.random.rand(1000, 1000)
c = np.dot(a, b)
cpu_time = time.time() - start_time
print(f'   çŸ©é˜µä¹˜æ³• (1000x1000): {cpu_time:.3f}ç§’')

# PyTorchæ€§èƒ½æµ‹è¯•
print('2. PyTorchæ€§èƒ½æµ‹è¯•:')
start_time = time.time()
x = torch.randn(1000, 1000)
y = torch.randn(1000, 1000)
z = torch.mm(x, y)
torch_time = time.time() - start_time
print(f'   PyTorchçŸ©é˜µä¹˜æ³•: {torch_time:.3f}ç§’')

# å†…å­˜åˆ†é…æµ‹è¯•
print('3. å†…å­˜æ€§èƒ½æµ‹è¯•:')
start_time = time.time()
large_array = np.zeros((10000, 1000))
memory_time = time.time() - start_time
print(f'   å¤§æ•°ç»„åˆ†é…: {memory_time:.3f}ç§’')

# æ€§èƒ½è¯„çº§
total_time = cpu_time + torch_time + memory_time
if total_time < 5:
    print('æ€§èƒ½è¯„çº§: ä¼˜ç§€')
elif total_time < 10:
    print('æ€§èƒ½è¯„çº§: è‰¯å¥½')
elif total_time < 20:
    print('æ€§èƒ½è¯„çº§: æ­£å¸¸')
else:
    print('æ€§èƒ½è¯„çº§: éœ€è¦ä¼˜åŒ–')

print(f'æ€»è®¡ç”¨æ—¶: {total_time:.3f}ç§’')
" > "$TEST_RESULTS_DIR/performance_test.txt"

# ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
echo "ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š..."
{
    echo "TradeMaster Docker é›†æˆæµ‹è¯•æŠ¥å‘Š"
    echo "=================================="
    echo "æµ‹è¯•æ—¶é—´: $(date)"
    echo "å®¹å™¨åç§°: $CONTAINER_NAME"
    echo ""
    
    echo "æµ‹è¯•ç»“æœæ¦‚è¿°:"
    echo "============="
    
    for test_file in "$TEST_RESULTS_DIR"/*.txt; do
        test_name=$(basename "$test_file" .txt)
        echo "- $test_name:"
        
        if grep -q "âœ…" "$test_file"; then
            success_count=$(grep -c "âœ…" "$test_file")
            echo "  æˆåŠŸé¡¹: $success_count"
        fi
        
        if grep -q "âŒ" "$test_file"; then
            failure_count=$(grep -c "âŒ" "$test_file")
            echo "  å¤±è´¥é¡¹: $failure_count"
        fi
        
        echo ""
    done
    
    echo "è¯¦ç»†ç»“æœ:"
    echo "========="
    
    for test_file in "$TEST_RESULTS_DIR"/*.txt; do
        echo ""
        echo "$(basename "$test_file" .txt):"
        echo "$(head -c 80 < /dev/zero | tr '\0' '-')"
        cat "$test_file"
        echo ""
    done
    
} > "$TEST_RESULTS_DIR/integration_test_report.txt"

echo "é›†æˆæµ‹è¯•å®Œæˆ!"
echo "æµ‹è¯•æŠ¥å‘Šä½ç½®: $TEST_RESULTS_DIR/integration_test_report.txt"
echo ""
echo "å¿«é€ŸæŸ¥çœ‹ç»“æœ:"
grep -E "(âœ…|âŒ|æ€§èƒ½è¯„çº§)" "$TEST_RESULTS_DIR"/*.txt
```

---

## ğŸ”„ æ¢å¤å’Œä¿®å¤æµç¨‹

### ğŸš¨ ç¾éš¾æ¢å¤æµç¨‹

#### å®Œæ•´ç³»ç»Ÿæ¢å¤
```bash
#!/bin/bash
# disaster_recovery.sh

set -euo pipefail

BACKUP_DIR="/opt/backups"
RECOVERY_LOG="/var/log/trademaster_recovery.log"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$RECOVERY_LOG"
}

log "å¼€å§‹ç¾éš¾æ¢å¤æµç¨‹..."

# æ­¥éª¤1: è¯„ä¼°æŸåæƒ…å†µ
log "è¯„ä¼°ç³»ç»ŸçŠ¶æ€..."
CONTAINER_EXISTS=$(docker ps -a | grep trademaster-container | wc -l)
IMAGE_EXISTS=$(docker images | grep trademaster | wc -l)

log "å®¹å™¨å­˜åœ¨: $CONTAINER_EXISTS, é•œåƒå­˜åœ¨: $IMAGE_EXISTS"

# æ­¥éª¤2: åœæ­¢æ‰€æœ‰ç›¸å…³æœåŠ¡
log "åœæ­¢æ‰€æœ‰TradeMasterç›¸å…³æœåŠ¡..."
docker stop trademaster-container 2>/dev/null || true
docker rm trademaster-container 2>/dev/null || true

# æ­¥éª¤3: æŸ¥æ‰¾æœ€æ–°å¤‡ä»½
log "æŸ¥æ‰¾æœ€æ–°å¤‡ä»½..."
LATEST_BACKUP=$(find "$BACKUP_DIR" -type d -name "*" | sort | tail -1)
log "æœ€æ–°å¤‡ä»½: $LATEST_BACKUP"

if [ -z "$LATEST_BACKUP" ] || [ ! -d "$LATEST_BACKUP" ]; then
    log "é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆå¤‡ä»½"
    exit 1
fi

# æ­¥éª¤4: æ¢å¤é•œåƒ
log "æ¢å¤Dockeré•œåƒ..."
if [ -f "$LATEST_BACKUP/trademaster_image.tar" ]; then
    docker load < "$LATEST_BACKUP/trademaster_image.tar"
    log "é•œåƒæ¢å¤å®Œæˆ"
else
    log "è­¦å‘Š: æœªæ‰¾åˆ°é•œåƒå¤‡ä»½ï¼Œå°è¯•é‡æ–°æ„å»º..."
    if [ -f "./Dockerfile" ]; then
        docker build -t trademaster:latest .
        log "é•œåƒé‡æ–°æ„å»ºå®Œæˆ"
    else
        log "é”™è¯¯: æ— æ³•æ„å»ºé•œåƒ"
        exit 1
    fi
fi

# æ­¥éª¤5: æ¢å¤æ•°æ®
log "æ¢å¤åº”ç”¨æ•°æ®..."
if [ -f "$LATEST_BACKUP/app_data.tar.gz" ]; then
    mkdir -p ./data_recovery
    tar -xzf "$LATEST_BACKUP/app_data.tar.gz" -C ./data_recovery
    
    # å¤‡ä»½å½“å‰æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if [ -d "./data" ]; then
        mv ./data "./data_backup_$(date +%Y%m%d_%H%M%S)"
    fi
    
    mv ./data_recovery ./data
    log "æ•°æ®æ¢å¤å®Œæˆ"
else
    log "è­¦å‘Š: æœªæ‰¾åˆ°æ•°æ®å¤‡ä»½"
fi

# æ­¥éª¤6: æ¢å¤é…ç½®
log "æ¢å¤é…ç½®æ–‡ä»¶..."
if [ -d "$LATEST_BACKUP/config" ]; then
    cp -r "$LATEST_BACKUP/config"/* ./
    log "é…ç½®æ¢å¤å®Œæˆ"
fi

# æ­¥éª¤7: å¯åŠ¨æœåŠ¡
log "å¯åŠ¨æ¢å¤åçš„æœåŠ¡..."
if [ -f "./start-container.bat" ] && command -v cmd.exe >/dev/null 2>&1; then
    # Windowsç¯å¢ƒ
    cmd.exe /c start-container.bat
elif [ -f "./start-container.sh" ]; then
    # Linuxç¯å¢ƒ
    ./start-container.sh
else
    # æ‰‹åŠ¨å¯åŠ¨
    docker run -d \
        --name trademaster-container \
        -p 8080:8080 \
        -p 8888:8888 \
        -p 5001:5000 \
        -v "$(pwd)/data:/app/data" \
        -v "$(pwd):/workspace" \
        --restart unless-stopped \
        trademaster:latest tail -f /dev/null
fi

# æ­¥éª¤8: éªŒè¯æ¢å¤
log "éªŒè¯æœåŠ¡æ¢å¤..."
sleep 30

if docker ps | grep -q trademaster-container; then
    log "âœ… å®¹å™¨å¯åŠ¨æˆåŠŸ"
else
    log "âŒ å®¹å™¨å¯åŠ¨å¤±è´¥"
    exit 1
fi

# å¥åº·æ£€æŸ¥
if curl -sf http://localhost:8080/health >/dev/null 2>&1; then
    log "âœ… æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡"
else
    log "âš ï¸ æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œä½†å®¹å™¨æ­£åœ¨è¿è¡Œ"
fi

# æ­¥éª¤9: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
log "æ•°æ®å®Œæ•´æ€§æ£€æŸ¥..."
docker exec trademaster-container python3 -c "
import os
import sys
sys.path.append('/home/TradeMaster')

try:
    import trademaster
    print('âœ… TradeMasteræ¨¡å—å¯ç”¨')
    
    # æ£€æŸ¥å…³é”®æ•°æ®æ–‡ä»¶
    data_files = [
        '/app/data/portfolio_management',
        '/app/data/algorithmic_trading'
    ]
    
    for path in data_files:
        if os.path.exists(path):
            file_count = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])
            print(f'âœ… {path}: {file_count} ä¸ªæ–‡ä»¶')
        else:
            print(f'âš ï¸ {path}: ç›®å½•ä¸å­˜åœ¨')
            
except Exception as e:
    print(f'âŒ æ•°æ®æ£€æŸ¥å¤±è´¥: {e}')
    sys.exit(1)
" 2>&1 | tee -a "$RECOVERY_LOG"

if [ ${PIPESTATUS[0]} -eq 0 ]; then
    log "âœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡"
else
    log "âŒ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥"
fi

log "ç¾éš¾æ¢å¤æµç¨‹å®Œæˆ!"
log "è¯·æ£€æŸ¥åº”ç”¨åŠŸèƒ½æ˜¯å¦æ­£å¸¸"

# å‘é€æ¢å¤é€šçŸ¥
if command -v mail >/dev/null 2>&1; then
    echo "TradeMasterç³»ç»Ÿå·²å®Œæˆç¾éš¾æ¢å¤ï¼Œè¯·æ£€æŸ¥åŠŸèƒ½" | mail -s "ç³»ç»Ÿæ¢å¤å®Œæˆ" admin@example.com
fi
```

#### é€æ­¥æ¢å¤æ£€æŸ¥æ¸…å•
```bash
#!/bin/bash
# recovery_checklist.sh

echo "TradeMaster æ¢å¤æ£€æŸ¥æ¸…å•"
echo "========================"

checks=0
passed=0

run_check() {
    local description="$1"
    local command="$2"
    
    checks=$((checks + 1))
    echo -n "[$checks] $description ... "
    
    if eval "$command" >/dev/null 2>&1; then
        echo "âœ… é€šè¿‡"
        passed=$((passed + 1))
        return 0
    else
        echo "âŒ å¤±è´¥"
        return 1
    fi
}

echo "åŸºç¡€è®¾æ–½æ£€æŸ¥:"
echo "============"

run_check "DockeræœåŠ¡è¿è¡Œ" "docker info"
run_check "å®¹å™¨æ­£åœ¨è¿è¡Œ" "docker ps | grep -q trademaster-container"
run_check "é•œåƒå­˜åœ¨" "docker images | grep -q trademaster"

echo ""
echo "ç½‘ç»œè¿æ¥æ£€æŸ¥:"
echo "============"

run_check "ç«¯å£8080å¯è®¿é—®" "curl -sf http://localhost:8080"
run_check "ç«¯å£8888å¯è®¿é—®" "curl -sf http://localhost:8888"
run_check "ç«¯å£5001å¯è®¿é—®" "curl -sf http://localhost:5001"

echo ""
echo "åº”ç”¨åŠŸèƒ½æ£€æŸ¥:"
echo "============"

run_check "Pythonç¯å¢ƒæ­£å¸¸" "docker exec trademaster-container python3 --version"
run_check "TradeMasteræ¨¡å—å¯å¯¼å…¥" "docker exec trademaster-container python3 -c 'import trademaster'"
run_check "æ•°æ®ç›®å½•å¯è®¿é—®" "docker exec trademaster-container ls /app/data"
run_check "å·¥ä½œç›®å½•æ˜ å°„æ­£å¸¸" "docker exec trademaster-container ls /workspace"

echo ""
echo "æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:"
echo "=============="

run_check "é…ç½®æ–‡ä»¶å­˜åœ¨" "docker exec trademaster-container ls /workspace/configs"
run_check "è®­ç»ƒæ•°æ®å­˜åœ¨" "docker exec trademaster-container ls /app/data/portfolio_management"
run_check "æ¨¡å‹ç›®å½•å­˜åœ¨" "docker exec trademaster-container ls /home/TradeMaster/trademaster"

echo ""
echo "æ€§èƒ½æ£€æŸ¥:"
echo "========"

run_check "CPUä½¿ç”¨ç‡æ­£å¸¸" "[ \$(docker stats --no-stream --format '{{.CPUPerc}}' trademaster-container | sed 's/%//') -lt 90 ]"
run_check "å†…å­˜ä½¿ç”¨ç‡æ­£å¸¸" "[ \$(docker stats --no-stream --format '{{.MemPerc}}' trademaster-container | sed 's/%//') -lt 90 ]"

echo ""
echo "æ£€æŸ¥ç»“æœæ€»ç»“:"
echo "============"
echo "æ€»æ£€æŸ¥é¡¹: $checks"
echo "é€šè¿‡é¡¹: $passed"
echo "å¤±è´¥é¡¹: $((checks - passed))"

if [ $passed -eq $checks ]; then
    echo "ğŸ‰ æ‰€æœ‰æ£€æŸ¥é¡¹éƒ½é€šè¿‡ï¼Œç³»ç»Ÿæ¢å¤æˆåŠŸï¼"
    exit 0
elif [ $passed -gt $((checks / 2)) ]; then
    echo "âš ï¸ å¤§éƒ¨åˆ†æ£€æŸ¥é¡¹é€šè¿‡ï¼Œä½†ä»æœ‰é—®é¢˜éœ€è¦è§£å†³"
    exit 1
else
    echo "âŒ å¤šé¡¹æ£€æŸ¥å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ’æŸ¥é—®é¢˜"
    exit 2
fi
```

---

## ğŸ“ è·å–æ”¯æŒ

### ğŸ†˜ æŠ€æœ¯æ”¯æŒæ¸ é“

- **GitHub Issues**: https://github.com/TradeMaster-NTU/TradeMaster/issues
- **é‚®ä»¶æ”¯æŒ**: TradeMaster.NTU@gmail.com  
- **ç¤¾åŒºè®¨è®º**: https://github.com/TradeMaster-NTU/TradeMaster/discussions

### ğŸ“‹ é—®é¢˜æŠ¥å‘Šæ¨¡æ¿

æäº¤é—®é¢˜æ—¶ï¼Œè¯·åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š

```
**ç¯å¢ƒä¿¡æ¯**:
- æ“ä½œç³»ç»Ÿ: [Windows/Linux/macOS]
- Dockerç‰ˆæœ¬: [docker --version]
- TradeMasterç‰ˆæœ¬: [é•œåƒæ ‡ç­¾]

**é—®é¢˜æè¿°**:
[è¯¦ç»†æè¿°é‡åˆ°çš„é—®é¢˜]

**é‡ç°æ­¥éª¤**:
1. [æ­¥éª¤1]
2. [æ­¥éª¤2]
3. [æ­¥éª¤3]

**æœŸæœ›è¡Œä¸º**:
[æè¿°æœŸæœ›çš„æ­£å¸¸è¡Œä¸º]

**é”™è¯¯æ—¥å¿—**:
```
[ç²˜è´´ç›¸å…³é”™è¯¯æ—¥å¿—]
```

**è¯Šæ–­ä¿¡æ¯**:
[è¿è¡Œè¯Šæ–­è„šæœ¬çš„è¾“å‡ºç»“æœ]
```

---

## ğŸ“„ ç‰ˆæœ¬ä¿¡æ¯

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0.0  
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ15æ—¥  
**é€‚ç”¨äº**: TradeMaster Docker v1.0+  
**ç»´æŠ¤å›¢é˜Ÿ**: TradeMaster Development Team