# TradeMaster Docker éƒ¨ç½²æœ€ä½³å®è·µæŒ‡å—

<div align="center">
    <h2>ğŸ† ç”Ÿäº§çº§éƒ¨ç½²æœ€ä½³å®è·µ</h2>
    <p>ä¼ä¸šçº§éƒ¨ç½²æ ‡å‡†ä¸è¿ç»´æŒ‡å—</p>
</div>

---

## ğŸ“‹ ç›®å½•

- [ğŸ—ï¸ æ¶æ„è®¾è®¡æœ€ä½³å®è·µ](#æ¶æ„è®¾è®¡æœ€ä½³å®è·µ)
- [ğŸ”’ å®‰å…¨æ€§æœ€ä½³å®è·µ](#å®‰å…¨æ€§æœ€ä½³å®è·µ)
- [âš¡ æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ](#æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ)
- [ğŸ“Š ç›‘æ§å’Œæ—¥å¿—æœ€ä½³å®è·µ](#ç›‘æ§å’Œæ—¥å¿—æœ€ä½³å®è·µ)
- [ğŸ”„ CI/CDæœ€ä½³å®è·µ](#cicdæœ€ä½³å®è·µ)
- [ğŸ’¾ æ•°æ®ç®¡ç†æœ€ä½³å®è·µ](#æ•°æ®ç®¡ç†æœ€ä½³å®è·µ)
- [ğŸ§ª æµ‹è¯•ç­–ç•¥æœ€ä½³å®è·µ](#æµ‹è¯•ç­–ç•¥æœ€ä½³å®è·µ)
- [ğŸ”§ è¿ç»´ç®¡ç†æœ€ä½³å®è·µ](#è¿ç»´ç®¡ç†æœ€ä½³å®è·µ)
- [ğŸ“ˆ æ‰©å±•æ€§æœ€ä½³å®è·µ](#æ‰©å±•æ€§æœ€ä½³å®è·µ)
- [âš ï¸ é£é™©ç®¡ç†æœ€ä½³å®è·µ](#é£é™©ç®¡ç†æœ€ä½³å®è·µ)

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡æœ€ä½³å®è·µ

### ğŸ¯ å®¹å™¨åŒ–è®¾è®¡åŸåˆ™

#### 1. å•ä¸€èŒè´£åŸåˆ™
```yaml
# æ¨è: æœåŠ¡åˆ†ç¦»æ¶æ„
version: '3.8'
services:
  trademaster-app:
    build: .
    environment:
      - ROLE=application
    depends_on:
      - trademaster-db
      - trademaster-cache
      
  trademaster-worker:
    build: .
    environment:
      - ROLE=worker
    depends_on:
      - trademaster-db
      - trademaster-cache
      
  trademaster-db:
    image: postgres:13
    environment:
      - POSTGRES_DB=trademaster
      
  trademaster-cache:
    image: redis:alpine
```

#### 2. é…ç½®å¤–éƒ¨åŒ–
```dockerfile
# Dockerfileæœ€ä½³å®è·µ
FROM ubuntu:20.04

# ä½¿ç”¨ARGè¿›è¡Œæ„å»ºæ—¶é…ç½®
ARG PYTHON_VERSION=3.8.10
ARG PYTORCH_VERSION=1.12.1

# ä½¿ç”¨ENVè¿›è¡Œè¿è¡Œæ—¶é…ç½®
ENV PYTHONPATH="/home/TradeMaster"
ENV WORKERS=4
ENV LOG_LEVEL=INFO

# é…ç½®æ–‡ä»¶æŒ‚è½½ç‚¹
VOLUME ["/app/config"]

# ä½¿ç”¨érootç”¨æˆ·
RUN useradd -m -u 1000 trademaster
USER trademaster

ENTRYPOINT ["/entrypoint.sh"]
```

#### 3. å¤šé˜¶æ®µæ„å»ºä¼˜åŒ–
```dockerfile
# å¤šé˜¶æ®µæ„å»ºç¤ºä¾‹
# é˜¶æ®µ1: æ„å»ºç¯å¢ƒ
FROM ubuntu:20.04 AS builder

RUN apt-get update && apt-get install -y \
    python3-dev \
    build-essential \
    cmake

COPY requirements.txt .
RUN pip install --user -r requirements.txt

# é˜¶æ®µ2: è¿è¡Œç¯å¢ƒ
FROM ubuntu:20.04 AS runtime

COPY --from=builder /root/.local /root/.local
COPY . /app

# ç¡®ä¿ç”¨æˆ·binåœ¨PATHä¸­
ENV PATH=/root/.local/bin:$PATH

WORKDIR /app
CMD ["python3", "main.py"]
```

### ğŸŒ ç½‘ç»œæ¶æ„è®¾è®¡

#### 1. ç½‘ç»œéš”ç¦»ç­–ç•¥
```bash
# åˆ›å»ºä¸“ç”¨ç½‘ç»œ
docker network create \
  --driver bridge \
  --subnet=172.20.0.0/16 \
  --gateway=172.20.0.1 \
  trademaster-net

# å‰ç«¯ç½‘ç»œï¼ˆå¯¹å¤–æš´éœ²ï¼‰
docker network create \
  --driver bridge \
  --subnet=172.21.0.0/16 \
  trademaster-frontend

# åç«¯ç½‘ç»œï¼ˆå†…éƒ¨é€šä¿¡ï¼‰
docker network create \
  --driver bridge \
  --subnet=172.22.0.0/16 \
  trademaster-backend
```

#### 2. è´Ÿè½½å‡è¡¡é…ç½®
```nginx
# nginx.conf - ç”Ÿäº§çº§é…ç½®
upstream trademaster_app {
    least_conn;
    server trademaster-app-1:8080 weight=3 max_fails=3 fail_timeout=30s;
    server trademaster-app-2:8080 weight=3 max_fails=3 fail_timeout=30s;
    server trademaster-app-3:8080 weight=2 max_fails=3 fail_timeout=30s backup;
}

server {
    listen 80;
    server_name trademaster.company.com;
    
    # å®‰å…¨å¤´è®¾ç½®
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains";
    
    # è¯·æ±‚é™åˆ¶
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    
    location / {
        limit_req zone=api burst=20 nodelay;
        
        proxy_pass http://trademaster_app;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 30s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # ç¼“å†²é…ç½®
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
    }
    
    # å¥åº·æ£€æŸ¥ç«¯ç‚¹
    location /health {
        access_log off;
        proxy_pass http://trademaster_app/health;
    }
    
    # é™æ€æ–‡ä»¶ä¼˜åŒ–
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

### ğŸ’¾ å­˜å‚¨æ¶æ„è®¾è®¡

#### 1. æ•°æ®å·ç®¡ç†ç­–ç•¥
```yaml
# docker-compose.production.yml
version: '3.8'
services:
  trademaster-app:
    image: trademaster:latest
    volumes:
      # é…ç½®åªè¯»æŒ‚è½½
      - ./config:/app/config:ro
      # æ•°æ®è¯»å†™æŒ‚è½½
      - trademaster-data:/app/data
      # æ—¥å¿—æŒ‚è½½
      - trademaster-logs:/app/logs
      # ä¸´æ—¶æ–‡ä»¶æŒ‚è½½
      - type: tmpfs
        target: /tmp
        tmpfs:
          size: 100M

volumes:
  trademaster-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/trademaster/data
      
  trademaster-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /var/log/trademaster
```

#### 2. å¤‡ä»½ç­–ç•¥è®¾è®¡
```bash
#!/bin/bash
# backup_strategy.sh

# 3-2-1å¤‡ä»½ç­–ç•¥å®æ–½
# 3ä¸ªå‰¯æœ¬ï¼Œ2ç§ä¸åŒåª’ä»‹ï¼Œ1ä¸ªå¼‚åœ°å¤‡ä»½

BACKUP_ROOT="/opt/backups/trademaster"
LOCAL_BACKUP="$BACKUP_ROOT/local"
REMOTE_BACKUP="$BACKUP_ROOT/remote"
OFFSITE_BACKUP="s3://company-backups/trademaster"

# æœ¬åœ°å¤‡ä»½ï¼ˆæ¯æ—¥ï¼‰
create_local_backup() {
    local date=$(date +%Y%m%d)
    local backup_dir="$LOCAL_BACKUP/$date"
    
    mkdir -p "$backup_dir"
    
    # åº”ç”¨æ•°æ®å¤‡ä»½
    docker exec trademaster-app tar czf - /app/data | \
        gzip > "$backup_dir/app_data.tar.gz"
    
    # æ•°æ®åº“å¤‡ä»½
    docker exec trademaster-db pg_dump -U trademaster trademaster | \
        gzip > "$backup_dir/database.sql.gz"
    
    # é…ç½®å¤‡ä»½
    tar czf "$backup_dir/config.tar.gz" /opt/trademaster/config/
    
    # éªŒè¯å¤‡ä»½å®Œæ•´æ€§
    verify_backup "$backup_dir"
}

# è¿œç¨‹å¤‡ä»½ï¼ˆæ¯å‘¨ï¼‰
create_remote_backup() {
    local week=$(date +%Y%U)
    local backup_dir="$REMOTE_BACKUP/$week"
    
    # åŒæ­¥åˆ°è¿œç¨‹NAS
    rsync -avz --delete "$LOCAL_BACKUP/" "$backup_dir/"
}

# å¼‚åœ°å¤‡ä»½ï¼ˆæ¯æœˆï¼‰
create_offsite_backup() {
    local month=$(date +%Y%m)
    
    # ä¸Šä¼ åˆ°äº‘å­˜å‚¨
    aws s3 sync "$LOCAL_BACKUP/" "$OFFSITE_BACKUP/$month/" \
        --storage-class GLACIER
}

# å¤‡ä»½éªŒè¯
verify_backup() {
    local backup_dir="$1"
    
    # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
    for file in "$backup_dir"/*.gz; do
        if ! gzip -t "$file"; then
            echo "é”™è¯¯: å¤‡ä»½æ–‡ä»¶æŸå - $file"
            return 1
        fi
    done
    
    echo "å¤‡ä»½éªŒè¯é€šè¿‡: $backup_dir"
}

# å¤‡ä»½æ¸…ç†ç­–ç•¥
cleanup_old_backups() {
    # ä¿ç•™æœ¬åœ°å¤‡ä»½30å¤©
    find "$LOCAL_BACKUP" -type d -mtime +30 -exec rm -rf {} +
    
    # ä¿ç•™è¿œç¨‹å¤‡ä»½12å‘¨
    find "$REMOTE_BACKUP" -type d -mtime +84 -exec rm -rf {} +
}
```

---

## ğŸ”’ å®‰å…¨æ€§æœ€ä½³å®è·µ

### ğŸ›¡ï¸ å®¹å™¨å®‰å…¨åŠ å›º

#### 1. æœ€å°æƒé™åŸåˆ™
```bash
# å®‰å…¨çš„å®¹å™¨å¯åŠ¨é…ç½®
docker run -d \
  --name trademaster-secure \
  # éç‰¹æƒæ¨¡å¼
  --user 1000:1000 \
  --security-opt no-new-privileges:true \
  # åªè¯»æ ¹æ–‡ä»¶ç³»ç»Ÿ
  --read-only \
  # ä¸´æ—¶æ–‡ä»¶ç³»ç»Ÿ
  --tmpfs /tmp:rw,size=100m,noexec,nosuid,nodev \
  --tmpfs /var/tmp:rw,size=50m,noexec,nosuid,nodev \
  # èƒ½åŠ›é™åˆ¶
  --cap-drop ALL \
  --cap-add NET_BIND_SERVICE \
  # èµ„æºé™åˆ¶
  --memory="8g" \
  --memory-swap="8g" \
  --cpus="4" \
  --pids-limit=1000 \
  # ç½‘ç»œéš”ç¦»
  --network trademaster-secure \
  trademaster:latest
```

#### 2. ç§˜å¯†ç®¡ç†
```yaml
# docker-compose.secrets.yml
version: '3.8'
services:
  trademaster-app:
    image: trademaster:latest
    secrets:
      - db_password
      - api_key
      - ssl_cert
    environment:
      # ä»ç§˜å¯†æ–‡ä»¶è¯»å–
      - DB_PASSWORD_FILE=/run/secrets/db_password
      - API_KEY_FILE=/run/secrets/api_key
    deploy:
      replicas: 3

secrets:
  db_password:
    external: true
    name: trademaster_db_password
  api_key:
    external: true  
    name: trademaster_api_key
  ssl_cert:
    file: ./ssl/trademaster.crt
```

#### 3. é•œåƒå®‰å…¨æ‰«æ
```bash
#!/bin/bash
# security_scan.sh

echo "å¼€å§‹é•œåƒå®‰å…¨æ‰«æ..."

# ä½¿ç”¨Trivyæ‰«ææ¼æ´
trivy image --severity HIGH,CRITICAL trademaster:latest

# ä½¿ç”¨Clairæ‰«æ
clair-scanner --ip $(hostname -I | awk '{print $1}') trademaster:latest

# ä½¿ç”¨Docker Scoutæ‰«æ
docker scout cves trademaster:latest

# ä½¿ç”¨Snykæ‰«æ
snyk container test trademaster:latest

echo "å®‰å…¨æ‰«æå®Œæˆï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Š"
```

### ğŸ” ç½‘ç»œå®‰å…¨

#### 1. TLS/SSLé…ç½®
```nginx
# SSLæœ€ä½³å®è·µé…ç½®
server {
    listen 443 ssl http2;
    server_name trademaster.company.com;
    
    # SSLè¯ä¹¦
    ssl_certificate /etc/ssl/certs/trademaster.crt;
    ssl_certificate_key /etc/ssl/private/trademaster.key;
    
    # SSLå®‰å…¨é…ç½®
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;
    
    # HSTS
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload";
    
    # OCSP Stapling
    ssl_stapling on;
    ssl_stapling_verify on;
    ssl_trusted_certificate /etc/ssl/certs/ca-certificates.crt;
    
    # å…¶ä»–å®‰å…¨å¤´
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Referrer-Policy "strict-origin-when-cross-origin";
    add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'";
}
```

#### 2. é˜²ç«å¢™é…ç½®
```bash
#!/bin/bash
# firewall_setup.sh

# UFWé˜²ç«å¢™é…ç½®
ufw --force reset
ufw default deny incoming
ufw default allow outgoing

# SSHè®¿é—®
ufw allow 22/tcp

# HTTP/HTTPSè®¿é—®
ufw allow 80/tcp
ufw allow 443/tcp

# Dockerå†…éƒ¨é€šä¿¡
ufw allow from 172.20.0.0/16
ufw allow from 172.21.0.0/16
ufw allow from 172.22.0.0/16

# ç›‘æ§æœåŠ¡
ufw allow from 10.0.0.0/8 to any port 9090  # Prometheus
ufw allow from 10.0.0.0/8 to any port 3000  # Grafana

# å¯ç”¨é˜²ç«å¢™
ufw --force enable

echo "é˜²ç«å¢™é…ç½®å®Œæˆ"
```

### ğŸ” å®¡è®¡å’Œåˆè§„

#### 1. å®¡è®¡æ—¥å¿—é…ç½®
```yaml
# docker-compose.audit.yml
version: '3.8'
services:
  trademaster-app:
    image: trademaster:latest
    logging:
      driver: syslog
      options:
        syslog-address: "tcp://localhost:514"
        tag: "trademaster-app"
        syslog-format: "rfc5424micro"
    
  auditd:
    image: linuxserver/auditd
    volumes:
      - /var/log/audit:/var/log/audit
      - ./audit-rules:/etc/audit/rules.d
    privileged: true
    
  log-aggregator:
    image: fluentd:latest
    ports:
      - "24224:24224"
    volumes:
      - ./fluentd.conf:/fluentd/etc/fluent.conf
      - /var/log:/var/log
```

#### 2. åˆè§„æ€§æ£€æŸ¥
```python
# compliance_checker.py
import docker
import json
import sys
from datetime import datetime

class ComplianceChecker:
    def __init__(self):
        self.client = docker.from_env()
        self.violations = []
        
    def check_container_security(self, container_name):
        """æ£€æŸ¥å®¹å™¨å®‰å…¨é…ç½®"""
        try:
            container = self.client.containers.get(container_name)
            config = container.attrs
            
            # æ£€æŸ¥ç‰¹æƒæ¨¡å¼
            if config['HostConfig']['Privileged']:
                self.violations.append({
                    'type': 'security',
                    'severity': 'high',
                    'message': 'å®¹å™¨è¿è¡Œåœ¨ç‰¹æƒæ¨¡å¼',
                    'container': container_name
                })
            
            # æ£€æŸ¥ç”¨æˆ·æƒé™
            if config['Config']['User'] == '' or config['Config']['User'] == 'root':
                self.violations.append({
                    'type': 'security',
                    'severity': 'medium',
                    'message': 'å®¹å™¨ä»¥rootç”¨æˆ·è¿è¡Œ',
                    'container': container_name
                })
            
            # æ£€æŸ¥åªè¯»æ ¹æ–‡ä»¶ç³»ç»Ÿ
            if not config['HostConfig']['ReadonlyRootfs']:
                self.violations.append({
                    'type': 'security',
                    'severity': 'medium',
                    'message': 'æ ¹æ–‡ä»¶ç³»ç»Ÿä¸æ˜¯åªè¯»',
                    'container': container_name
                })
            
            # æ£€æŸ¥èµ„æºé™åˆ¶
            if not config['HostConfig']['Memory']:
                self.violations.append({
                    'type': 'resource',
                    'severity': 'low',
                    'message': 'æœªè®¾ç½®å†…å­˜é™åˆ¶',
                    'container': container_name
                })
                
        except Exception as e:
            print(f"æ£€æŸ¥å®¹å™¨ {container_name} æ—¶å‡ºé”™: {e}")
    
    def check_image_security(self, image_name):
        """æ£€æŸ¥é•œåƒå®‰å…¨"""
        try:
            image = self.client.images.get(image_name)
            
            # æ£€æŸ¥é•œåƒæ ‡ç­¾
            if not image.tags or 'latest' in image.tags[0]:
                self.violations.append({
                    'type': 'security',
                    'severity': 'medium',
                    'message': 'ä½¿ç”¨äº†latestæ ‡ç­¾æˆ–æ— æ ‡ç­¾é•œåƒ',
                    'image': image_name
                })
            
            # æ£€æŸ¥é•œåƒå¤§å°
            size_mb = image.attrs['Size'] / (1024 * 1024)
            if size_mb > 1000:  # å¤§äº1GB
                self.violations.append({
                    'type': 'performance',
                    'severity': 'low',
                    'message': f'é•œåƒè¿‡å¤§: {size_mb:.1f}MB',
                    'image': image_name
                })
                
        except Exception as e:
            print(f"æ£€æŸ¥é•œåƒ {image_name} æ—¶å‡ºé”™: {e}")
    
    def generate_report(self):
        """ç”Ÿæˆåˆè§„æ€§æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_violations': len(self.violations),
            'violations_by_severity': {
                'high': len([v for v in self.violations if v['severity'] == 'high']),
                'medium': len([v for v in self.violations if v['severity'] == 'medium']),
                'low': len([v for v in self.violations if v['severity'] == 'low'])
            },
            'violations': self.violations
        }
        
        return report
    
    def run_compliance_check(self, container_name, image_name):
        """è¿è¡Œå®Œæ•´åˆè§„æ€§æ£€æŸ¥"""
        print("å¼€å§‹åˆè§„æ€§æ£€æŸ¥...")
        
        self.check_container_security(container_name)
        self.check_image_security(image_name)
        
        report = self.generate_report()
        
        # ä¿å­˜æŠ¥å‘Š
        with open('/app/data/compliance_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        # æ‰“å°æ‘˜è¦
        print(f"åˆè§„æ€§æ£€æŸ¥å®Œæˆ:")
        print(f"  æ€»è¿è§„é¡¹: {report['total_violations']}")
        print(f"  é«˜å±: {report['violations_by_severity']['high']}")
        print(f"  ä¸­å±: {report['violations_by_severity']['medium']}")
        print(f"  ä½å±: {report['violations_by_severity']['low']}")
        
        return report['violations_by_severity']['high'] == 0

if __name__ == "__main__":
    checker = ComplianceChecker()
    is_compliant = checker.run_compliance_check("trademaster-container", "trademaster:latest")
    sys.exit(0 if is_compliant else 1)
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

### ğŸš€ å®¹å™¨æ€§èƒ½ä¼˜åŒ–

#### 1. èµ„æºé…ç½®ä¼˜åŒ–
```yaml
# docker-compose.performance.yml
version: '3.8'
services:
  trademaster-app:
    image: trademaster:latest
    deploy:
      resources:
        limits:
          # CPUé™åˆ¶ - ä½¿ç”¨å°æ•°è¡¨ç¤ºæ ¸å¿ƒæ•°
          cpus: '4.0'
          # å†…å­˜é™åˆ¶
          memory: 16G
          # è®¾å¤‡é™åˆ¶
          devices:
            - /dev/nvidia0:/dev/nvidia0  # GPUæ”¯æŒ
        reservations:
          # é¢„ç•™èµ„æº
          cpus: '2.0'
          memory: 8G
    
    # ç³»ç»Ÿä¼˜åŒ–
    sysctls:
      # ç½‘ç»œä¼˜åŒ–
      net.core.rmem_max: 134217728
      net.core.wmem_max: 134217728
      net.ipv4.tcp_rmem: "4096 87380 134217728"
      net.ipv4.tcp_wmem: "4096 65536 134217728"
      # å†…å­˜ä¼˜åŒ–
      vm.swappiness: 10
      vm.dirty_ratio: 15
    
    # ç¯å¢ƒä¼˜åŒ–
    environment:
      # Pythonä¼˜åŒ–
      - PYTHONOPTIMIZE=1
      - PYTHONUNBUFFERED=1
      # NumPy/SciPyä¼˜åŒ–
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4
      - OPENBLAS_NUM_THREADS=4
      # PyTorchä¼˜åŒ–
      - TORCH_HOME=/app/cache/torch
```

#### 2. é•œåƒä¼˜åŒ–ç­–ç•¥
```dockerfile
# ä¼˜åŒ–çš„Dockerfile
FROM ubuntu:20.04 as base

# ä½¿ç”¨å›½å†…é•œåƒæº
RUN sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list

# ä¸€æ¬¡æ€§å®‰è£…ï¼Œå‡å°‘å±‚æ•°
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3=3.8.10-0ubuntu1~20.04 \
    python3-pip=20.0.2-5ubuntu1 \
    python3-dev=3.8.2-0ubuntu2 \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# PythonåŒ…ç¼“å­˜ä¼˜åŒ–
FROM base as python-deps
COPY requirements-docker.txt /tmp/
RUN pip3 install --no-cache-dir --user -r /tmp/requirements-docker.txt

# åº”ç”¨å±‚
FROM base as app
COPY --from=python-deps /root/.local /root/.local
ENV PATH=/root/.local/bin:$PATH

# åº”ç”¨ä»£ç 
COPY . /app
WORKDIR /app

# érootç”¨æˆ·
RUN useradd -m -u 1000 trademaster && \
    chown -R trademaster:trademaster /app
USER trademaster

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

ENTRYPOINT ["/app/entrypoint.sh"]
```

#### 3. ç¼“å­˜ç­–ç•¥
```python
# caching_strategy.py
import redis
import json
import hashlib
import pickle
from functools import wraps
from datetime import datetime, timedelta

class CacheManager:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(
            host=redis_host, 
            port=redis_port, 
            decode_responses=False
        )
        
    def cache_result(self, ttl=3600, key_prefix="tm"):
        """ç»“æœç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._generate_cache_key(
                    func.__name__, args, kwargs, key_prefix
                )
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    return pickle.loads(cached_result)
                
                # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = func(*args, **kwargs)
                self.redis_client.setex(
                    cache_key, ttl, pickle.dumps(result)
                )
                
                return result
            return wrapper
        return decorator
    
    def cache_dataframe(self, df, key, ttl=1800):
        """DataFrameç¼“å­˜"""
        # å‹ç¼©å­˜å‚¨
        compressed_df = df.to_pickle(compression='gzip')
        self.redis_client.setex(key, ttl, compressed_df)
    
    def get_cached_dataframe(self, key):
        """è·å–ç¼“å­˜çš„DataFrame"""
        import pandas as pd
        cached_data = self.redis_client.get(key)
        if cached_data:
            return pd.read_pickle(
                io.BytesIO(cached_data), 
                compression='gzip'
            )
        return None
    
    def _generate_cache_key(self, func_name, args, kwargs, prefix):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = {
            'function': func_name,
            'args': args,
            'kwargs': sorted(kwargs.items())
        }
        key_string = json.dumps(key_data, sort_keys=True)
        key_hash = hashlib.md5(key_string.encode()).hexdigest()
        return f"{prefix}:{func_name}:{key_hash}"
    
    def warm_up_cache(self):
        """ç¼“å­˜é¢„çƒ­"""
        print("å¼€å§‹ç¼“å­˜é¢„çƒ­...")
        
        # é¢„åŠ è½½å¸¸ç”¨æ•°æ®
        common_symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA']
        for symbol in common_symbols:
            # é¢„åŠ è½½è‚¡ç¥¨æ•°æ®
            self._preload_stock_data(symbol)
        
        print("ç¼“å­˜é¢„çƒ­å®Œæˆ")
    
    def _preload_stock_data(self, symbol):
        """é¢„åŠ è½½è‚¡ç¥¨æ•°æ®"""
        # å®ç°é¢„åŠ è½½é€»è¾‘
        pass

# ä½¿ç”¨ç¤ºä¾‹
cache_manager = CacheManager()

@cache_manager.cache_result(ttl=3600)
def expensive_calculation(param1, param2):
    """è€—æ—¶è®¡ç®—å‡½æ•°"""
    import time
    time.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return param1 * param2

# é¢„çƒ­ç¼“å­˜
cache_manager.warm_up_cache()
```

### ğŸ’¾ æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

#### 1. PostgreSQLä¼˜åŒ–é…ç½®
```sql
-- postgresql.conf ä¼˜åŒ–é…ç½®
-- å†…å­˜é…ç½®
shared_buffers = 4GB                    -- 25% of total RAM
effective_cache_size = 12GB             -- 75% of total RAM
work_mem = 256MB                        -- For complex queries
maintenance_work_mem = 1GB              -- For maintenance operations

-- å†™å…¥ä¼˜åŒ–
wal_buffers = 64MB
checkpoint_completion_target = 0.9
checkpoint_timeout = 15min
max_wal_size = 4GB
min_wal_size = 1GB

-- è¿æ¥ä¼˜åŒ–
max_connections = 200
shared_preload_libraries = 'pg_stat_statements'

-- æŸ¥è¯¢ä¼˜åŒ–
random_page_cost = 1.1                  -- For SSD
effective_io_concurrency = 200          -- For SSD
```

#### 2. æ•°æ®åº“è¿æ¥æ± 
```python
# db_pool.py
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool
import threading
import time

class DatabaseManager:
    def __init__(self, db_url, pool_size=20, max_overflow=30):
        self.engine = create_engine(
            db_url,
            poolclass=QueuePool,
            pool_size=pool_size,
            max_overflow=max_overflow,
            pool_pre_ping=True,
            pool_recycle=3600,  # 1å°æ—¶å›æ”¶è¿æ¥
            echo=False
        )
        self._local = threading.local()
    
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        if not hasattr(self._local, 'connection'):
            self._local.connection = self.engine.connect()
        return self._local.connection
    
    def execute_query(self, query, params=None):
        """æ‰§è¡ŒæŸ¥è¯¢"""
        conn = self.get_connection()
        try:
            result = conn.execute(query, params or {})
            return result.fetchall()
        except Exception as e:
            conn.rollback()
            raise e
    
    def close_connection(self):
        """å…³é—­è¿æ¥"""
        if hasattr(self._local, 'connection'):
            self._local.connection.close()
            delattr(self._local, 'connection')

# å•ä¾‹æ¨¡å¼
db_manager = None

def get_db_manager():
    global db_manager
ç°åœ¨åˆ›å»ºéƒ¨ç½²æœ€ä½³å®è·µå’Œç»´æŠ¤æŒ‡å—ï¼š

<write_to_file>
<path>docs/BEST_PRACTICES_GUIDE.md</path>
<content># TradeMaster Docker éƒ¨ç½²æœ€ä½³å®è·µæŒ‡å—

<div align="center">
    <h2>ğŸ† ç”Ÿäº§çº§éƒ¨ç½²æœ€ä½³å®è·µ</h2>
    <p>ä»å¼€å‘åˆ°ç”Ÿäº§çš„å®Œæ•´æœ€ä½³å®è·µæŒ‡å—</p>
</div>

---

## ğŸ“‹ ç›®å½•

- [ğŸ—ï¸ æ¶æ„è®¾è®¡æœ€ä½³å®è·µ](#æ¶æ„è®¾è®¡æœ€ä½³å®è·µ)
- [ğŸ”’ å®‰å…¨é…ç½®æœ€ä½³å®è·µ](#å®‰å…¨é…ç½®æœ€ä½³å®è·µ)
- [âš¡ æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ](#æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ)
- [ğŸ“Š ç›‘æ§å’Œæ—¥å¿—æœ€ä½³å®è·µ](#ç›‘æ§å’Œæ—¥å¿—æœ€ä½³å®è·µ)
- [ğŸ”„ CI/CDæœ€ä½³å®è·µ](#cicdæœ€ä½³å®è·µ)
- [ğŸ’¾ æ•°æ®ç®¡ç†æœ€ä½³å®è·µ](#æ•°æ®ç®¡ç†æœ€ä½³å®è·µ)
- [ğŸ› ï¸ è¿ç»´ç®¡ç†æœ€ä½³å®è·µ](#è¿ç»´ç®¡ç†æœ€ä½³å®è·µ)
- [ğŸŒ å¤šç¯å¢ƒéƒ¨ç½²æœ€ä½³å®è·µ](#å¤šç¯å¢ƒéƒ¨ç½²æœ€ä½³å®è·µ)

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡æœ€ä½³å®è·µ

### ğŸ¯ å®¹å™¨åŒ–æ¶æ„åŸåˆ™

#### å•ä¸€èŒè´£åŸåˆ™
```yaml
# æ¨èï¼šæŒ‰åŠŸèƒ½åˆ†ç¦»æœåŠ¡
services:
  trademaster-web:
    image: trademaster:web
    ports: ["8080:8080"]
    
  trademaster-api:
    image: trademaster:api
    ports: ["5000:5000"]
    
  trademaster-worker:
    image: trademaster:worker
    deploy:
      replicas: 3

# é¿å…ï¼šå•å®¹å™¨æ‰¿æ‹…æ‰€æœ‰åŠŸèƒ½
```

#### æ— çŠ¶æ€è®¾è®¡
```bash
# æ­£ç¡®ï¼šä½¿ç”¨å¤–éƒ¨å­˜å‚¨
docker run -d \
  --name trademaster-app \
  -v /shared/data:/app/data \
  -e DATABASE_URL=postgresql://host/db \
  -e REDIS_URL=redis://host:6379 \
  trademaster:latest

# é”™è¯¯ï¼šåœ¨å®¹å™¨å†…å­˜å‚¨çŠ¶æ€
docker run -d \
  --name trademaster-app \
  trademaster:latest
# ï¼ˆæ•°æ®ä¼šåœ¨å®¹å™¨é‡å¯æ—¶ä¸¢å¤±ï¼‰
```

### ğŸ”§ å¤šå±‚æ¶æ„è®¾è®¡

#### ä¸‰å±‚æ¶æ„å®ç°
```yaml
# docker-compose.production.yml
version: '3.8'

services:
  # å‰ç«¯å±‚
  nginx:
    image: nginx:alpine
    ports: ["80:80", "443:443"]
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/ssl/certs
    depends_on: [trademaster-web]
    networks: [frontend]

  # åº”ç”¨å±‚
  trademaster-web:
    image: trademaster:latest
    deploy:
      replicas: 3
      resources:
        limits: {memory: 4G, cpus: '2'}
        reservations: {memory: 2G, cpus: '1'}
    environment:
      - NODE_ENV=production
      - DATABASE_URL=${DATABASE_URL}
    networks: [frontend, backend]
    depends_on: [postgres, redis]

  # æ•°æ®å±‚
  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: trademaster
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks: [backend]

  redis:
    image: redis:alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks: [backend]

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
    internal: true

volumes:
  postgres_data:
  redis_data:
```

### ğŸ“¦ é•œåƒæ„å»ºæœ€ä½³å®è·µ

#### å¤šé˜¶æ®µæ„å»º
```dockerfile
# Dockerfile.optimized
# æ„å»ºé˜¶æ®µ
FROM python:3.8-slim as builder

WORKDIR /build
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

COPY . .
RUN python setup.py build

# è¿è¡Œé˜¶æ®µ
FROM python:3.8-slim as runtime

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r trademaster && useradd -r -g trademaster trademaster

# å¤åˆ¶æ„å»ºäº§ç‰©
COPY --from=builder /root/.local /home/trademaster/.local
COPY --from=builder /build/dist /app

# è®¾ç½®æƒé™
RUN chown -R trademaster:trademaster /app
USER trademaster

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8080/health || exit 1

WORKDIR /app
CMD ["python", "-m", "trademaster.server"]
```

#### é•œåƒä¼˜åŒ–ç­–ç•¥
```bash
#!/bin/bash
# image_optimization.sh

echo "Dockeré•œåƒä¼˜åŒ–è„šæœ¬"

# 1. æ¸…ç†æœªä½¿ç”¨çš„é•œåƒ
docker image prune -f

# 2. å¤šæ¶æ„æ„å»º
docker buildx build \
  --platform linux/amd64,linux/arm64 \
  --tag trademaster:multi-arch \
  --push .

# 3. é•œåƒå®‰å…¨æ‰«æ
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
  aquasec/trivy image trademaster:latest

# 4. é•œåƒä½“ç§¯åˆ†æ
docker run --rm -it \
  -v /var/run/docker.sock:/var/run/docker.sock \
  wagoodman/dive:latest trademaster:latest
```

---

## ğŸ”’ å®‰å…¨é…ç½®æœ€ä½³å®è·µ

### ğŸ›¡ï¸ å®¹å™¨å®‰å…¨åŠ å›º

#### æœ€å°æƒé™åŸåˆ™
```bash
# ç”Ÿäº§ç¯å¢ƒå®‰å…¨å¯åŠ¨
docker run -d \
  --name trademaster-secure \
  --user 1000:1000 \
  --security-opt no-new-privileges:true \
  --security-opt apparmor:docker-default \
  --cap-drop ALL \
  --cap-add NET_BIND_SERVICE \
  --read-only \
  --tmpfs /tmp:rw,size=100m,noexec,nosuid,nodev \
  --tmpfs /var/tmp:rw,size=50m,noexec,nosuid,nodev \
  -p 8080:8080 \
  -v /opt/trademaster/data:/app/data:rw \
  -v /opt/trademaster/config:/app/config:ro \
  --restart unless-stopped \
  --memory="4g" \
  --cpus="2.0" \
  --log-driver json-file \
  --log-opt max-size=10m \
  --log-opt max-file=3 \
  trademaster:latest
```

#### ç½‘ç»œå®‰å…¨é…ç½®
```yaml
# docker-compose.secure.yml
version: '3.8'

services:
  trademaster:
    image: trademaster:latest
    networks:
      - internal
      - external
    ports:
      - "127.0.0.1:8080:8080"  # åªç»‘å®šåˆ°æœ¬åœ°æ¥å£

  nginx:
    image: nginx:alpine
    networks:
      - external
    ports:
      - "80:80"
      - "443:443"
    configs:
      - source: nginx_config
        target: /etc/nginx/nginx.conf

networks:
  external:
    driver: bridge
  internal:
    driver: bridge
    internal: true  # å†…éƒ¨ç½‘ç»œï¼Œæ— å¤–ç½‘è®¿é—®

configs:
  nginx_config:
    file: ./nginx/secure.conf
```

### ğŸ” æœºå¯†ä¿¡æ¯ç®¡ç†

#### Docker Secrets é…ç½®
```yaml
# docker-compose.secrets.yml
version: '3.8'

services:
  trademaster:
    image: trademaster:latest
    secrets:
      - db_password
      - api_key
      - ssl_cert
    environment:
      - DATABASE_PASSWORD_FILE=/run/secrets/db_password
      - API_KEY_FILE=/run/secrets/api_key

secrets:
  db_password:
    file: ./secrets/db_password.txt
  api_key:
    external: true
  ssl_cert:
    file: ./secrets/ssl_cert.pem
```

#### ç¯å¢ƒå˜é‡æœ€ä½³å®è·µ
```bash
# .env.production
# ä½¿ç”¨å¼ºå¯†ç 
DATABASE_PASSWORD=$(openssl rand -base64 32)
API_SECRET_KEY=$(openssl rand -hex 64)
JWT_SECRET=$(openssl rand -base64 32)

# é™åˆ¶æ•°æ®åº“è¿æ¥
DATABASE_MAX_CONNECTIONS=20
DATABASE_CONNECTION_TIMEOUT=30

# å¯ç”¨å®‰å…¨ç‰¹æ€§
SECURE_SSL_REDIRECT=true
SESSION_COOKIE_SECURE=true
CSRF_COOKIE_SECURE=true

# æ—¥å¿—å®‰å…¨é…ç½®
LOG_LEVEL=INFO
LOG_SANITIZE_SECRETS=true
```

### ğŸ” å®‰å…¨ç›‘æ§

#### å®¹å™¨è¿è¡Œæ—¶å®‰å…¨ç›‘æ§
```python
# security_monitor.py
import docker
import psutil
import logging
from datetime import datetime, timedelta

class SecurityMonitor:
    def __init__(self):
        self.client = docker.from_env()
        self.logger = logging.getLogger(__name__)
        
    def monitor_container_security(self, container_name):
        """ç›‘æ§å®¹å™¨å®‰å…¨çŠ¶æ€"""
        try:
            container = self.client.containers.get(container_name)
            
            # æ£€æŸ¥å®¹å™¨é…ç½®
            config_issues = self.check_container_config(container)
            
            # æ£€æŸ¥è¿è¡Œæ—¶çŠ¶æ€
            runtime_issues = self.check_runtime_security(container)
            
            # ç”Ÿæˆå®‰å…¨æŠ¥å‘Š
            self.generate_security_report(container_name, config_issues, runtime_issues)
            
        except docker.errors.NotFound:
            self.logger.error(f"å®¹å™¨ {container_name} æœªæ‰¾åˆ°")
    
    def check_container_config(self, container):
        """æ£€æŸ¥å®¹å™¨é…ç½®å®‰å…¨æ€§"""
        issues = []
        
        # æ£€æŸ¥æ˜¯å¦ä»¥rootç”¨æˆ·è¿è¡Œ
        if container.attrs['Config']['User'] == '' or container.attrs['Config']['User'] == 'root':
            issues.append("å®¹å™¨ä»¥rootç”¨æˆ·è¿è¡Œ")
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç‰¹æƒæ¨¡å¼
        if container.attrs['HostConfig']['Privileged']:
            issues.append("å®¹å™¨å¯ç”¨äº†ç‰¹æƒæ¨¡å¼")
        
        # æ£€æŸ¥ç«¯å£ç»‘å®š
        port_bindings = container.attrs['HostConfig']['PortBindings']
        for port, bindings in (port_bindings or {}).items():
            for binding in bindings:
                if binding['HostIp'] == '' or binding['HostIp'] == '0.0.0.0':
                    issues.append(f"ç«¯å£ {port} ç»‘å®šåˆ°æ‰€æœ‰æ¥å£")
        
        # æ£€æŸ¥å·æŒ‚è½½
        mounts = container.attrs['Mounts']
        for mount in mounts:
            if mount['Source'].startswith('/'):
                if mount['Mode'] == 'rw' and mount['Source'] in ['/etc', '/var', '/usr']:
                    issues.append(f"æ•æ„Ÿç›®å½• {mount['Source']} ä»¥è¯»å†™æ¨¡å¼æŒ‚è½½")
        
        return issues
    
    def check_runtime_security(self, container):
        """æ£€æŸ¥è¿è¡Œæ—¶å®‰å…¨çŠ¶æ€"""
        issues = []
        
        # æ£€æŸ¥è¿›ç¨‹åˆ—è¡¨
        try:
            processes = container.top()['Processes']
            
            # æ£€æŸ¥å¼‚å¸¸è¿›ç¨‹
            suspicious_processes = []
            for process in processes:
                cmd = ' '.join(process[7:]) if len(process) > 7 else ''
                
                # æ£€æŸ¥æ˜¯å¦æœ‰shellè¿›ç¨‹
                if any(shell in cmd.lower() for shell in ['bash', 'sh', 'zsh', 'fish']):
                    suspicious_processes.append(cmd)
                
                # æ£€æŸ¥ç½‘ç»œå·¥å…·
                if any(tool in cmd.lower() for tool in ['nc', 'netcat', 'wget', 'curl']):
                    suspicious_processes.append(cmd)
            
            if suspicious_processes:
                issues.append(f"å‘ç°å¯ç–‘è¿›ç¨‹: {suspicious_processes}")
                
        except Exception as e:
            self.logger.warning(f"æ— æ³•æ£€æŸ¥è¿›ç¨‹åˆ—è¡¨: {e}")
        
        return issues
    
    def generate_security_report(self, container_name, config_issues, runtime_issues):
        """ç”Ÿæˆå®‰å…¨æŠ¥å‘Š"""
        report = {
            'container': container_name,
            'timestamp': datetime.now().isoformat(),
            'config_issues': config_issues,
            'runtime_issues': runtime_issues,
            'risk_level': self.calculate_risk_level(config_issues, runtime_issues)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f'/var/log/security_report_{container_name}.json', 'w') as f:
            import json
            json.dump(report, f, indent=2)
        
        # å‘é€å‘Šè­¦
        if report['risk_level'] == 'HIGH':
            self.send_security_alert(report)
    
    def calculate_risk_level(self, config_issues, runtime_issues):
        """è®¡ç®—é£é™©ç­‰çº§"""
        total_issues = len(config_issues) + len(runtime_issues)
        
        high_risk_keywords = ['root', 'ç‰¹æƒ', '0.0.0.0', '/etc', 'shell']
        high_risk_count = sum(1 for issue in config_issues + runtime_issues 
                             if any(keyword in issue.lower() for keyword in high_risk_keywords))
        
        if high_risk_count > 0 or total_issues > 5:
            return 'HIGH'
        elif total_issues > 2:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def send_security_alert(self, report):
        """å‘é€å®‰å…¨å‘Šè­¦"""
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶ã€Slackã€å¾®ä¿¡ç­‰å‘Šè­¦æ¸ é“
        self.logger.critical(f"å®‰å…¨å‘Šè­¦: å®¹å™¨ {report['container']} å­˜åœ¨é«˜é£é™©å®‰å…¨é—®é¢˜")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = SecurityMonitor()
    monitor.monitor_container_security('trademaster-container')
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

### ğŸš€ å®¹å™¨æ€§èƒ½ä¼˜åŒ–

#### èµ„æºé…ç½®æœ€ä½³å®è·µ
```yaml
# docker-compose.performance.yml
version: '3.8'

services:
  trademaster:
    image: trademaster:latest
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
      restart_policy:
        condition: unless-stopped
        delay: 5s
        max_attempts: 3
        window: 120s
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    environment:
      - PYTHONOPTIMIZE=1
      - PYTHONUNBUFFERED=1
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4
      - NUMEXPR_NUM_THREADS=4
    
    # ä½¿ç”¨é«˜æ€§èƒ½å­˜å‚¨
    volumes:
      - type: volume
        source: trademaster_data
        target: /app/data
        volume:
          driver: local
          driver_opts:
            type: tmpfs
            device: tmpfs
            o: size=2g,uid=1000
    
    # ç½‘ç»œä¼˜åŒ–
    networks:
      - trademaster_net
    
    # æ—¥å¿—ä¼˜åŒ–
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

networks:
  trademaster_net:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: 9000  # Jumbo frames

volumes:
  trademaster_data:
    driver: local
    driver_opts:
      type: none
      device: /opt/ssd/trademaster
      o: bind
```

#### CPUå’Œå†…å­˜ä¼˜åŒ–
```python
# performance_optimizer.py
import os
import psutil
import torch
import numpy as np
from multiprocessing import cpu_count

class PerformanceOptimizer:
    def __init__(self):
        self.cpu_count = cpu_count()
        self.memory_total = psutil.virtual_memory().total
        
    def optimize_environment(self):
        """ä¼˜åŒ–è¿è¡Œç¯å¢ƒ"""
        # CPUä¼˜åŒ–
        self.optimize_cpu()
        
        # å†…å­˜ä¼˜åŒ–
        self.optimize_memory()
        
        # PyTorchä¼˜åŒ–
        self.optimize_pytorch()
        
        # NumPyä¼˜åŒ–
        self.optimize_numpy()
    
    def optimize_cpu(self):
        """CPUä¼˜åŒ–é…ç½®"""
        # è®¾ç½®CPUäº²å’Œæ€§
        if hasattr(os, 'sched_setaffinity'):
            # ç»‘å®šåˆ°æ€§èƒ½æ ¸å¿ƒ
            performance_cpus = list(range(min(4, self.cpu_count)))
            os.sched_setaffinity(0, performance_cpus)
            print(f"CPUäº²å’Œæ€§è®¾ç½®åˆ°æ ¸å¿ƒ: {performance_cpus}")
        
        # è®¾ç½®çº¿ç¨‹æ•°
        optimal_threads = min(4, self.cpu_count)
        os.environ['OMP_NUM_THREADS'] = str(optimal_threads)
        os.environ['MKL_NUM_THREADS'] = str(optimal_threads)
        os.environ['NUMEXPR_NUM_THREADS'] = str(optimal_threads)
        
        print(f"çº¿ç¨‹æ•°è®¾ç½®ä¸º: {optimal_threads}")
    
    def optimize_memory(self):
        """å†…å­˜ä¼˜åŒ–é…ç½®"""
        # è®¾ç½®å†…å­˜æ˜ å°„é˜ˆå€¼
        if self.memory_total > 8 * 1024**3:  # 8GB+
            os.environ['MALLOC_MMAP_THRESHOLD_'] = '131072'
            os.environ['MALLOC_TRIM_THRESHOLD_'] = '524288'
        
        # Pythonå†…å­˜ä¼˜åŒ–
        import gc
        gc.set_threshold(700, 10, 10)  # è°ƒæ•´åƒåœ¾å›æ”¶é˜ˆå€¼
        
        print("å†…å­˜ä¼˜åŒ–é…ç½®å®Œæˆ")
    
    def optimize_pytorch(self):
        """PyTorchæ€§èƒ½ä¼˜åŒ–"""
        if torch.cuda.is_available():
            # GPUä¼˜åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.enabled = True
            
            # è®¾ç½®å†…å­˜æ± 
            torch.cuda.set_per_process_memory_fraction(0.8)
            
            print("PyTorch GPUä¼˜åŒ–å®Œæˆ")
        else:
            # CPUä¼˜åŒ–
            torch.set_num_threads(min(4, self.cpu_count))
            torch.set_num_interop_threads(min(2, self.cpu_count // 2))
            
            print("PyTorch CPUä¼˜åŒ–å®Œæˆ")
    
    def optimize_numpy(self):
        """NumPyæ€§èƒ½ä¼˜åŒ–"""
        # è®¾ç½®BLASçº¿ç¨‹æ•°
        os.environ['OPENBLAS_NUM_THREADS'] = str(min(4, self.cpu_count))
        os.environ['VECLIB_MAXIMUM_THREADS'] = str(min(4, self.cpu_count))
        
        # å¯ç”¨å¤šçº¿ç¨‹
        np.seterr(all='raise')  # ä¸¥æ ¼é”™è¯¯å¤„ç†
        
        print("NumPyä¼˜åŒ–å®Œæˆ")
    
    def benchmark_performance(self):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        # CPUåŸºå‡†æµ‹è¯•
        cpu_score = self.cpu_benchmark()
        
        # å†…å­˜åŸºå‡†æµ‹è¯•
        memory_score = self.memory_benchmark()
        
        # æ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•
        dl_score = self.deep_learning_benchmark()
        
        # ç»¼åˆè¯„åˆ†
        overall_score = (cpu_score + memory_score + dl_score) / 3
        
        report = {
            'cpu_score': cpu_score,
            'memory_score': memory_score,
            'deep_learning_score': dl_score,
            'overall_score': overall_score,
            'performance_grade': self.get_performance_grade(overall_score)
        }
        
        return report
    
    def cpu_benchmark(self):
        """CPUåŸºå‡†æµ‹è¯•"""
        import time
        
        start_time = time.time()
        
        # çŸ©é˜µä¹˜æ³•æµ‹è¯•
        a = np.random.rand(1000, 1000)
        b = np.random.rand(1000, 1000)
        c = np.dot(a, b)
        
        cpu_time = time.time() - start_time
        
        # è¯„åˆ†ï¼ˆç§’è½¬æ¢ä¸ºåˆ†æ•°ï¼Œè¶Šå¿«åˆ†æ•°è¶Šé«˜ï¼‰
        cpu_score = max(0, 100 - cpu_time * 20)
        
        print(f"CPUåŸºå‡†æµ‹è¯•: {cpu_time:.3f}ç§’, è¯„åˆ†: {cpu_score:.1f}")
        return cpu_score
    
    def memory_benchmark(self):
        """å†…å­˜åŸºå‡†æµ‹è¯•"""
        import time
        
        start_time = time.time()
        
        # å¤§æ•°ç»„åˆ†é…å’Œæ“ä½œ
        arrays = []
        for i in range(10):
            arr = np.random.rand(5000, 1000)
            arrays.append(arr * 2 + 1)
        
        # æ¸…ç†
        del arrays
        import gc
        gc.collect()
        
        memory_time = time.time() - start_time
        memory_score = max(0, 100 - memory_time * 10)
        
        print(f"å†…å­˜åŸºå‡†æµ‹è¯•: {memory_time:.3f}ç§’, è¯„åˆ†: {memory_score:.1f}")
        return memory_score
    
    def deep_learning_benchmark(self):
        """æ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•"""
        import time
        
        start_time = time.time()
        
        # PyTorchåŸºå‡†æµ‹è¯•
        if torch.cuda.is_available():
            device = torch.device('cuda')
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            z = torch.mm(x, y)
            torch.cuda.synchronize()
        else:
            x = torch.randn(1000, 1000)
            y = torch.randn(1000, 1000)
            z = torch.mm(x, y)
        
        dl_time = time.time() - start_time
        dl_score = max(0, 100 - dl_time * 30)
        
        print(f"æ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•: {dl_time:.3f}ç§’, è¯„åˆ†: {dl_score:.1f}")
        return dl_score
    
    def get_performance_grade(self, score):
        """è·å–æ€§èƒ½ç­‰çº§"""
        if score >= 90:
            return 'A+ (ä¼˜ç§€)'
        elif score >= 80:
            return 'A (è‰¯å¥½)'
        elif score >= 70:
            return 'B+ (ä¸­ç­‰åä¸Š)'
        elif score >= 60:
            return 'B (ä¸­ç­‰)'
        elif score >= 50:
            return 'C (ä¸€èˆ¬)'
        else:
            return 'D (éœ€è¦ä¼˜åŒ–)'

# å®¹å™¨å¯åŠ¨æ—¶è‡ªåŠ¨ä¼˜åŒ–
def auto_optimize():
    """è‡ªåŠ¨æ€§èƒ½ä¼˜åŒ–"""
    optimizer = PerformanceOptimizer()
    optimizer.optimize_environment()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = optimizer.benchmark_performance()
    
    print(f"\næ€§èƒ½ä¼˜åŒ–å®Œæˆ!")
    print(f"ç»¼åˆè¯„åˆ†: {results['overall_score']:.1f}")
    print(f"æ€§èƒ½ç­‰çº§: {results['performance_grade']}")
    
    return results

if __name__ == "__main__":
    auto_optimize()
```

### ğŸ“Š å­˜å‚¨ä¼˜åŒ–

#### æ•°æ®å·æ€§èƒ½ä¼˜åŒ–
```bash
#!/bin/bash
# storage_optimization.sh

echo "å­˜å‚¨æ€§èƒ½ä¼˜åŒ–é…ç½®..."

# 1. åˆ›å»ºé«˜æ€§èƒ½æ•°æ®å·
docker volume create \
  --driver local \
  --opt type=tmpfs \
  --opt device=tmpfs \
  --opt o=size=4g,uid=1000 \
  trademaster-cache

# 2. ä½¿ç”¨SSDå­˜å‚¨
docker volume create \
  --driver local \
  --opt type=none \
  --opt device=/opt/ssd/trademaster \
  --opt o=bind \
  trademaster-data

# 3. é…ç½®I/Oè°ƒåº¦å™¨
echo "noop" > /sys/block/sda/queue/scheduler  # SSDä¼˜åŒ–
echo "deadline" > /sys/block/sdb/queue/scheduler  # HDDä¼˜åŒ–

# 4. ä¼˜åŒ–æ–‡ä»¶ç³»ç»Ÿå‚æ•°
mount -o remount,noatime,nodiratime /opt/ssd

echo "å­˜å‚¨ä¼˜åŒ–å®Œæˆ"
```

#### æ•°æ®åº“è¿æ¥ä¼˜åŒ–
```python
# database_optimizer.py
import asyncio
import asyncpg
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

class DatabaseOptimizer:
    def __init__(self):
        self.connection_pools = {}
    
    def create_optimized_engine(self, database_url):
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åº“å¼•æ“"""
        engine = create_engine(
            database_url,
            poolclass=QueuePool,
            pool_size=20,                    # è¿æ¥æ± å¤§å°
            max_overflow=30,                 # æœ€å¤§æº¢å‡ºè¿æ¥
            pool_pre_ping=True,              # è¿æ¥é¢„æ£€
            pool_recycle=3600,               # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆç§’ï¼‰
            pool_timeout=30,                 # è·å–è¿æ¥è¶…æ—¶
            echo=False,                      # ç”Ÿäº§ç¯å¢ƒå…³é—­SQLæ—¥å¿—
            execution_options={
                'isolation_level': 'READ_COMMITTED',
                'autocommit': False
            }
        )
        
        return engine
    
    async def create_async_pool(self, database_url):
        """åˆ›å»ºå¼‚æ­¥è¿æ¥æ± """
        pool = await asyncpg.create_pool(
            database_url,
            min_size=10,                     # æœ€å°è¿æ¥æ•°
            max_size=50,                     # æœ€å¤§è¿æ¥æ•°
            max_queries=50000,               # æ¯è¿æ¥æœ€å¤§æŸ¥è¯¢æ•°
            max_inactive_connection_lifetime=300,  # éæ´»è·ƒè¿æ¥ç”Ÿå‘½å‘¨æœŸ
            command_timeout=60,              # å‘½ä»¤è¶…æ—¶
            server_settings={
                'jit': 'off',                # å…³é—­JITç¼–è¯‘
                'application_name': 'trademaster'
            }
        )
        
        return pool
    
    def optimize_queries(self):
        """æŸ¥è¯¢ä¼˜åŒ–å»ºè®®"""
        optimizations = [
            "ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢",
            "é¿å…SELECT *ï¼ŒåªæŸ¥è¯¢éœ€è¦çš„å­—æ®µ",
            "ä½¿ç”¨LIMITé™åˆ¶ç»“æœé›†å¤§å°",
            "ä½¿ç”¨æ‰¹é‡æ“ä½œå‡å°‘æ•°æ®åº“å¾€è¿”",
            "å®ç°æŸ¥è¯¢ç»“æœç¼“å­˜",
            "ä½¿ç”¨è¿æ¥æ± å¤ç”¨è¿æ¥",
            "å®šæœŸåˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’"
        ]
        
        return optimizations
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—æœ€ä½³å®è·µ

### ğŸ“ˆ å…¨æ ˆç›‘æ§æ–¹æ¡ˆ

#### Prometheus + Grafana ç›‘æ§æ ˆ
```yaml
# monitoring-stack.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:v2.30.0
    container_name: prometheus
    ports: ["9090:9090"]
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks: [monitoring]

  grafana:
    image: grafana/grafana:8.2.0
    container_name: grafana
    ports: ["3000:3000"]
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    networks: [monitoring]

  alertmanager:
    image: prom/alertmanager:v0.23.0
    container_name: alertmanager
    ports: ["9093:9093"]
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    networks: [monitoring]

  node-exporter:
    image: prom/node-exporter:v1.2.0
    container_name: node-exporter
    ports: ["9100:9100"]
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
    networks: [monitoring]

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.40.0
    container_name: cadvisor
    ports: ["8081:8080"]
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    networks: [monitoring]

networks:
  monitoring:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
```

#### è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡
```python
# custom_metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import functools
import psutil
import threading

class TradeMasterMetrics:
    def __init__(self):
        # ä¸šåŠ¡æŒ‡æ ‡
        self.trade_requests = Counter('trademaster_trade_requests_total', 
                                    'Total trade requests', ['method', 'status'])
        self.trade_latency = Histogram('trademaster_trade_duration_seconds',
                                     'Trade request latency')
        self.active_positions = Gauge('trademaster_active_positions',
                                    'Number of active positions')
        self.portfolio_value = Gauge('trademaster_portfolio_value',
                                   'Current portfolio value')
        
        # ç³»ç»ŸæŒ‡æ ‡
        self.memory_usage = Gauge('trademaster_memory_usage_bytes',
                                'Memory usage in bytes')
        self.cpu_usage = Gauge('trademaster_cpu_usage_percent',
                             'CPU usage percentage')
        self.disk_usage = Gauge('trademaster_disk_usage_percent',
                              'Disk usage percentage')
        
        # æ¨¡å‹æŒ‡æ ‡
        self.model_predictions = Counter('trademaster_model_predictions_total',
                                       'Total model predictions', ['model', 'outcome'])
        self.model_accuracy = Gauge('trademaster_model_accuracy',
                                  'Model accuracy', ['model'])
        
        self.start_system_metrics_collection()
    
    def track_trade(self, method, status='success'):
        """è·Ÿè¸ªäº¤æ˜“è¯·æ±‚"""
        self.trade_requests.labels(method=method, status=status).inc()
    
    def time_trade_execution(self, func):
        """äº¤æ˜“æ‰§è¡Œæ—¶é—´è£…é¥°å™¨"""
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            with self.trade_latency.time():
                return func(*args, **kwargs)
        return wrapper
    
    def update_portfolio_metrics(self, positions, total_value):
        """æ›´æ–°æŠ•èµ„ç»„åˆæŒ‡æ ‡"""
        self.active_positions.set(len(positions))
        self.portfolio_value.set(total_value)
    
    def track_model_prediction(self, model_name, prediction, actual=None):
        """è·Ÿè¸ªæ¨¡å‹é¢„æµ‹"""
        if actual is not None:
            outcome = 'correct' if prediction == actual else 'incorrect'
            self.model_predictions.labels(model=model_name, outcome=outcome).inc()
    
    def update_model_accuracy(self, model_name, accuracy):
        """æ›´æ–°æ¨¡å‹å‡†ç¡®ç‡"""
        self.model_accuracy.labels(model=model_name).set(accuracy)
    
    def start_system_metrics_collection(self):
        """å¯åŠ¨ç³»ç»ŸæŒ‡æ ‡æ”¶é›†"""
        def collect_system_metrics():
            while True:
                # CPUä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=1)
                self.cpu_usage.set(cpu_percent)
                
                # å†…å­˜ä½¿ç”¨
                memory = psutil.virtual_memory()
                self.memory_usage.set(memory.used)
                
                # ç£ç›˜ä½¿ç”¨
                disk = psutil.disk_usage('/app/data')
                self.disk_usage.set(disk.percent)
                
                time.sleep(30)  # æ¯30ç§’æ”¶é›†ä¸€æ¬¡
        
        collector_thread = threading.Thread(target=collect_system_metrics, daemon=True)
        collector_thread.start()
    
    def start_metrics_server(self, port=8000):
        """å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨"""
        start_http_server(port)
        print(f"PrometheusæŒ‡æ ‡æœåŠ¡å™¨å¯åŠ¨åœ¨ç«¯å£ {port}")

# å…¨å±€æŒ‡æ ‡å®ä¾‹
metrics = TradeMasterMetrics()

# ä½¿ç”¨ç¤ºä¾‹
@metrics.time_trade_execution
def execute_trade(symbol, quantity):
    """æ‰§è¡Œäº¤æ˜“"""
    try:
        # äº¤æ˜“é€»è¾‘
        result = perform_trade(symbol, quantity)
        metrics.track_trade('buy', 'success')
        return result
    except Exception as e:
        metrics.track_trade('buy', 'failed')
        raise e

def update_portfolio_status():
    """æ›´æ–°æŠ•èµ„ç»„åˆçŠ¶æ€"""
    positions = get_current_positions()
    total_value = calculate_portfolio_value(positions)
    metrics.update_portfolio_metrics(positions, total_value)

# å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨
if __name__ == "__main__":
    metrics.start_metrics_server(8000)
```

### ğŸ“ ç»“æ„åŒ–æ—¥å¿—

#### æ—¥å¿—é…ç½®æœ€ä½³å®è·µ
```python
# logging_config.py
import logging
import logging.handlers
import json
import time
from datetime import datetime
import traceback
import os

class StructuredLogger:
    def __init__(self, name, level=logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        
        # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
        if not self.logger.handlers:
            self.setup_handlers()
    
    def setup_handlers(self):
        """è®¾ç½®æ—¥å¿—å¤„ç†å™¨"""
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = self.ColoredFormatter()
        console_handler.setFormatter(console_formatter)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.handlers.RotatingFileHandler(
            '/app/logs/trademaster.log',
            maxBytes=100*1024*1024,  # 100MB
            backupCount=10
        )
        file_handler.setLevel(logging.DEBUG)
        file_formatter = self.JSONFormatter()
        file_handler.setFormatter(file_formatter)
        
        # é”™è¯¯æ–‡ä»¶å¤„ç†å™¨
        error_handler = logging.handlers.RotatingFileHandler(
            '/app/logs/trademaster_error.log',
            maxBytes=50*1024*1024,   # 50MB
            backupCount=5
        )
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(file_formatter)
        
        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)
        self.logger.addHandler(error_handler)
    
    class JSONFormatter(logging.Formatter):
        """JSONæ ¼å¼åŒ–å™¨"""
        def format(self, record):
            log_entry = {
                'timestamp': datetime.utcfromtimestamp(record.created).isoformat() + 'Z',
                'level': record.levelname,
                'logger': record.name,
                'message': record.getMessage(),
                'module': record.module,
                'function': record.funcName,
                'line': record.lineno,
                'process_id': os.getpid(),
                'thread_id': record.thread
            }
            
            # æ·»åŠ å¼‚å¸¸ä¿¡æ¯
            if record.exc_info:
                log_entry['exception'] = {
                    'type': record.exc_info[0].__name__,
                    'message': str(record.exc_info[1]),
                    'traceback': traceback.format_exception(*record.exc_info)
                }
            
            # æ·»åŠ è‡ªå®šä¹‰å­—æ®µ
            if hasattr(record, 'extra_fields'):
                log_entry.update(record.extra_fields)
            
            return json.dumps(log_entry, ensure_ascii=False)
    
    class ColoredFormatter(logging.Formatter):
        """å½©è‰²æ§åˆ¶å°æ ¼å¼åŒ–å™¨"""
        COLORS = {
            'DEBUG': '\033[36m',    # é’è‰²
            'INFO': '\033[32m',     # ç»¿è‰²
            'WARNING': '\033[33m',  # é»„è‰²
            'ERROR': '\033[31m',    # çº¢è‰²
            'CRITICAL': '\033[35m', # ç´«è‰²
        }
        RESET = '\033[0m'
        
        def format(self, record):
            color = self.COLORS.get(record.levelname, self.RESET)
            record.levelname = f"{color}{record.levelname}{self.RESET}"
            
            formatter = logging.Formatter(
                '%(asctime)s | %(levelname)s | %(name)s | %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            return formatter.format(record)
    
    def log_with_context(self, level, message, **context):
        """å¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—è®°å½•"""
        extra = {'extra_fields': context}
        self.logger.log(level, message, extra=extra)
    
    def log_trade_event(self, event_type, symbol, quantity, price, **kwargs):
        """è®°å½•äº¤æ˜“äº‹ä»¶"""
        context = {
            'event_type': 'trade',
            'trade_event': event_type,
            'symbol': symbol,
            'quantity': quantity,
            'price': price,
            **kwargs
        }
        self.log_with_context(logging.INFO, f"äº¤æ˜“äº‹ä»¶: {event_type}", **context)
    
    def log_model_event(self, model_name, event_type, **kwargs):
        """è®°å½•æ¨¡å‹äº‹ä»¶"""
        context = {
            'event_type': 'model',
            'model_name': model_name,
            'model_event': event_type,
            **kwargs
        }
        self.log_with_context(logging.INFO, f"æ¨¡å‹äº‹ä»¶: {event_type}", **context)
    
    def log_performance_metrics(self, metrics_dict):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        context = {
            'event_type': 'performance',
            **metrics_dict
        }
        self.log_with_context(logging.INFO, "æ€§èƒ½æŒ‡æ ‡æ›´æ–°", **context)

# å…¨å±€æ—¥å¿—å®ä¾‹
logger = StructuredLogger('trademaster')

# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    # åŸºæœ¬æ—¥å¿—
    logger.logger.info("åº”ç”¨å¯åŠ¨")
    
    # å¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—
    logger.log_with_context(
        logging.INFO, 
        "ç”¨æˆ·ç™»å½•",
        user_id=12345,
        ip_address="192.168.1.100",
        user_agent="TradeMaster/1.0"
    )
    
    # äº¤æ˜“äº‹ä»¶æ—¥å¿—
    logger.log_trade_event(
        event_type="buy_order",
        symbol="AAPL",
        quantity=100,
        price=150.25,
        order_id="ORD123456",
        strategy="momentum"
    )
    
    # æ¨¡å‹äº‹ä»¶æ—¥å¿—
    logger.log_model_event(
        model_name="lstm_predictor",
        event_type="prediction",
        confidence=0.85,
        prediction="bullish",
        features_count=20
    )
    
    # æ€§èƒ½æŒ‡æ ‡æ—¥å¿—
    logger.log_performance_metrics({
        'cpu_usage': 45.2,
        'memory_usage': 2.1,
        'response_time': 0.123,
        'throughput': 1250
    })

if __name__ == "__main__":
    example_usage()
```

---

## ğŸ”„ CI/CDæœ€ä½³å®è·µ

### ğŸš€ GitHub Actionså·¥ä½œæµ

#### å®Œæ•´çš„CI/CDç®¡é“
```yaml
# .github/workflows/docker-cicd.yml
name: TradeMaster Docker CI/CD

on:
  push:
    branches: [main, develop]
    tags: ['v*']
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8 black isort
    
    - name: Code quality checks
      run: |
        # ä»£ç æ ¼å¼æ£€æŸ¥
        black --check .
        isort --check-only .
        flake8 . --max-line-length=88 --extend-ignore=E203,W503
    
    - name: Run unit tests
      run: |
        pytest tests/ -v --cov=trademaster --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  build:
    needs: [test, security-scan]
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64,linux/arm64

  deploy-staging:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: staging
    
    steps:
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # è¿™é‡Œæ·»åŠ éƒ¨ç½²åˆ°stagingç¯å¢ƒçš„è„šæœ¬
        
  deploy-production:
    needs: build
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    environment: production
    
    steps:
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # è¿™é‡Œæ·»åŠ éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒçš„è„šæœ¬

  integration-test:
    needs: build
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run integration tests
      run: |
        docker-compose -f docker-compose.test.yml up --build --abort-on-container-exit
        docker-compose -f docker-compose.test.yml down
```

#### å¤šç¯å¢ƒéƒ¨ç½²é…ç½®
```yaml
# docker-compose.staging.yml
version: '3.8'

services:
  trademaster:
    image: ghcr.io/trademaster-ntu/trademaster:develop
    environment:
      - NODE_ENV=staging
      - DATABASE_URL=${STAGING_DATABASE_URL}
      - API_BASE_URL=https://staging-api.trademaster.com
      - LOG_LEVEL=DEBUG
    deploy:
      replicas: 2
      resources:
        limits: {memory: 4G, cpus: '2'}
    networks: [staging]

networks:
  staging:
    external: true

---
# docker-compose.production.yml
version: '3.8'

services:
  trademaster:
    image: ghcr.io/trademaster-ntu/trademaster:latest
    environment:
      - NODE_ENV=production
      - DATABASE_URL=${PRODUCTION_DATABASE_URL}
      - API_BASE_URL=https://api.trademaster.com
      - LOG_LEVEL=INFO
    deploy:
      replicas: 5
      resources:
        limits: {memory: 8G, cpus: '4'}
      update_config:
        parallelism: 2
        delay: 10s
        failure_action: rollback
        order: start-first
      rollback_config:
        parallelism: 1
        delay: 5s
    networks: [production]

networks:
  production:
    external: true
```

---

## ğŸ’¾ æ•°æ®ç®¡ç†æœ€ä½³å®è·µ

### ğŸ“Š æ•°æ®å¤‡ä»½ç­–ç•¥

#### è‡ªåŠ¨åŒ–å¤‡ä»½ç³»ç»Ÿ
```bash
#!/bin/bash
# automated_backup.sh

set -euo pipefail

# é…ç½®
BACKUP_DIR="/opt/backups/trademaster"
RETENTION_DAYS=30
S3_BUCKET="trademaster-backups"
ENCRYPTION_KEY="/etc/trademaster/backup.key"

# æ—¥å¿—å‡½æ•°
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a /var/log/trademaster_backup.log
}

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

log "å¼€å§‹TradeMasteræ•°æ®å¤‡ä»½..."

# è·å–å½“å‰æ—¶é—´æˆ³
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="$BACKUP_DIR/backup_$TIMESTAMP"

mkdir -p "$BACKUP_PATH"

# 1. å¤‡ä»½å®¹å™¨æ•°æ®
log "å¤‡ä»½å®¹å™¨æ•°æ®å·..."
docker run --rm \
  -v trademaster_data:/data \
  -v "$BACKUP_PATH":/backup \
  alpine:latest \
  tar czf /backup/data_volume.tar.gz -C /data .

# 2. å¤‡ä»½æ•°æ®åº“
log "å¤‡ä»½PostgreSQLæ•°æ®åº“..."
docker exec postgres-container pg_dump \
  -U trademaster \
  -d trademaster \
  --no-owner \
  --no-privileges \
  | gzip > "$BACKUP_PATH/database.sql.gz"

# 3. å¤‡ä»½é…ç½®æ–‡ä»¶
log "å¤‡ä»½é…ç½®æ–‡ä»¶..."
tar czf "$BACKUP_PATH/config.tar.gz" \
  -C /opt/trademaster \
  config/ \
  docker-compose.yml \
  .env

# 4. å¤‡ä»½ä»£ç ä»“åº“
log "å¤‡ä»½ä»£ç ä»“åº“..."
if [ -d "/opt/trademaster/.git" ]; then
    cd /opt/trademaster
    git bundle create "$BACKUP_PATH/repository.bundle" --all
fi

# 5. åˆ›å»ºå¤‡ä»½æ¸…å•
log "åˆ›å»ºå¤‡ä»½æ¸…å•..."
cat > "$BACKUP_PATH/manifest.json" << EOF
{
  "timestamp": "$TIMESTAMP",
  "backup_type": "full",
  "components": [
    "data_volume",
    "database", 
    "config",
    "repository"
  ],
  "retention_until": "$(date -d "+$RETENTION_DAYS days" '+%Y-%m-%d')",
  "size_mb": $(du -sm "$BACKUP_PATH" | cut -f1)
}
EOF

# 6. åŠ å¯†å¤‡ä»½
if [ -f "$ENCRYPTION_KEY" ]; then
    log "åŠ å¯†å¤‡ä»½æ–‡ä»¶..."
    tar czf - -C "$BACKUP_DIR" "backup_$TIMESTAMP" | \
    openssl enc -aes-256-cbc -salt -k "$(cat $ENCRYPTION_KEY)" \
    > "$BACKUP_DIR/backup_${TIMESTAMP}_encrypted.tar.gz"
    
    # åˆ é™¤æœªåŠ å¯†ç‰ˆæœ¬
    rm -rf "$BACKUP_PATH"
    BACKUP_FILE="backup_${TIMESTAMP}_encrypted.tar.gz"
else
    # å‹ç¼©å¤‡ä»½
    tar czf "$BACKUP_DIR/backup_${TIMESTAMP}.tar.gz" -C "$BACKUP_DIR" "backup_$TIMESTAMP"
    rm -rf "$BACKUP_PATH"
    BACKUP_FILE="backup_${TIMESTAMP}.tar.gz"
fi

# 7. ä¸Šä¼ åˆ°äº‘å­˜å‚¨
if command -v aws >/dev/null 2>&1 && [ -n "${S3_BUCKET:-}" ]; then
    log "ä¸Šä¼ å¤‡ä»½åˆ°S3..."
    aws s3 cp "$BACKUP_DIR/$BACKUP_FILE" "s3://$S3_BUCKET/$(date +%Y/%m)/"
    
    # éªŒè¯ä¸Šä¼ 
    if aws s3 ls "s3://$S3_BUCKET/$(date +%Y/%m)/$BACKUP_FILE" >/dev/null; then
        log "S3ä¸Šä¼ æˆåŠŸ"
    else
        log "é”™è¯¯: S3ä¸Šä¼ å¤±è´¥"
        exit 1
    fi
fi

# 8. æ¸…ç†æ—§å¤‡ä»½
log "æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶..."
find "$BACKUP_DIR" -name "backup_*.tar.gz" -mtime +$RETENTION_DAYS -delete

# 9. éªŒè¯å¤‡ä»½å®Œæ•´æ€§
log "éªŒè¯å¤‡ä»½å®Œæ•´æ€§..."
if tar tzf "$BACKUP_DIR/$BACKUP_FILE" >/dev/null 2>&1; then
    log "å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡"
else
    log "é”™è¯¯: å¤‡ä»½æ–‡ä»¶æŸå"
    exit 1
fi

# 10. å‘é€å¤‡ä»½æŠ¥å‘Š
BACKUP_SIZE=$(du -sh "$BACKUP_DIR/$BACKUP_FILE" | cut -f1)
log "å¤‡ä»½å®Œæˆ! æ–‡ä»¶å¤§å°: $BACKUP_SIZE"

# å‘é€é€šçŸ¥
if command -v mail >/dev/null 2>&1; then
    echo "TradeMasterå¤‡ä»½å®Œæˆ
    
æ—¶é—´: $TIMESTAMP
å¤§å°: $BACKUP_SIZE
ä½ç½®: $BACKUP_DIR/$BACKUP_FILE" | \
    mail -s "TradeMasterå¤‡ä»½å®Œæˆ" admin@example.com
fi

log "å¤‡ä»½æµç¨‹å®Œæˆ"
```

#### ç¾éš¾æ¢å¤æµ‹è¯•
```bash
#!/bin/bash
# disaster_recovery_test.sh

set -euo pipefail

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log "å¼€å§‹ç¾éš¾æ¢å¤æµ‹è¯•..."

# 1. åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
log "åˆ›å»ºéš”ç¦»çš„æµ‹è¯•ç¯å¢ƒ..."
docker network create trademaster-test 2>/dev/null || true

# 2. é€‰æ‹©æœ€æ–°å¤‡ä»½
BACKUP_DIR="/opt/backups/trademaster"
LATEST_BACKUP=$(ls -t "$BACKUP_DIR"/backup_*.tar.gz | head -1)

if [ -z "$LATEST_BACKUP" ]; then
    log "é”™è¯¯: æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶"
    exit 1
fi

log "ä½¿ç”¨å¤‡ä»½æ–‡ä»¶: $LATEST_BACKUP"

# 3. è§£å‹å¤‡ä»½
TEST_DIR="/tmp/trademaster_recovery_test"
mkdir -p "$TEST_DIR"

if [[ "$LATEST_BACKUP" == *"_encrypted"* ]]; then
    # è§£å¯†å¤‡ä»½
    openssl enc -aes-256-cbc -d -salt -k "$(cat /etc/trademaster/backup.key)" \
        -in "$LATEST_BACKUP" | tar xzf - -C "$TEST_DIR"
else
    tar xzf "$LATEST_BACKUP" -C "$TEST_DIR"
fi

# 4. æ¢å¤æ•°æ®åº“
log "æ¢å¤æµ‹è¯•æ•°æ®åº“..."
docker run -d \
    --name postgres-test \
    --network trademaster-test \
    -e POSTGRES_DB=trademaster_test \
    -e POSTGRES_USER=test \
    -e POSTGRES_PASSWORD=test123 \
    postgres:13

# ç­‰å¾…æ•°æ®åº“å¯åŠ¨
sleep 30

# å¯¼å…¥æ•°æ®åº“å¤‡ä»½
zcat "$TEST_DIR"/backup_*/database.sql.gz | \
docker exec -i postgres-test psql -U test -d trademaster_test

# 5. æ¢å¤åº”ç”¨æ•°æ®
log "æ¢å¤åº”ç”¨æ•°æ®..."
docker volume create trademaster-test-data

docker run --rm \
    -v trademaster-test-data:/data \
    -v "$TEST_DIR"/backup_*:/backup \
    alpine:latest \
    tar xzf /backup/data_volume.tar.gz -C /data

# 6. å¯åŠ¨æµ‹è¯•åº”ç”¨
log "å¯åŠ¨æµ‹è¯•åº”ç”¨..."
docker run -d \
    --name trademaster-test \
    --network trademaster-test \
    -e DATABASE_URL=postgresql://test:test123@postgres-test/trademaster_test \
    -e NODE_ENV=test \
    -v trademaster-test-data:/app/data \
    -p 18080:8080 \
    trademaster:latest

# 7. å¥åº·æ£€æŸ¥
log "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
sleep 60  # ç­‰å¾…åº”ç”¨å¯åŠ¨

# æ£€æŸ¥åº”ç”¨å“åº”
if curl -f http://localhost:18080/health >/dev/null 2>&1; then
    log "âœ… åº”ç”¨å¥åº·æ£€æŸ¥é€šè¿‡"
else
    log "âŒ åº”ç”¨å¥åº·æ£€æŸ¥å¤±è´¥"
    RECOVERY_SUCCESS=false
fi

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
if docker exec trademaster-test python3 -c "
import psycopg2
try:
    conn = psycopg2.connect('postgresql://test:test123@postgres-test/trademaster_test')
    cur = conn.cursor()
    cur.execute('SELECT COUNT(*) FROM information_schema.tables;')
    count = cur.fetchone()[0]
    print(f'æ•°æ®åº“è¡¨æ•°é‡: {count}')
    if count > 0:
        print('âœ… æ•°æ®åº“æ¢å¤æˆåŠŸ')
    else:
        print('âŒ æ•°æ®åº“æ¢å¤å¤±è´¥')
        exit(1)
except Exception as e:
    print(f'âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}')
    exit(1)
"; then
    log "âœ… æ•°æ®åº“è¿æ¥æµ‹è¯•é€šè¿‡"
else
    log "âŒ æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥"
    RECOVERY_SUCCESS=false
fi

# 8. åŠŸèƒ½æµ‹è¯•
log "æ‰§è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•..."
if docker exec trademaster-test python3 -c "
import sys
sys.path.append('/home/TradeMaster')
try:
    import trademaster
    print('âœ… TradeMasteræ¨¡å—å¯¼å…¥æˆåŠŸ')
    
    # æµ‹è¯•æ•°æ®è®¿é—®
    import os
    if os.path.exists('/app/data'):
        file_count = len(os.listdir('/app/data'))
        print(f'âœ… æ•°æ®æ–‡ä»¶æ¢å¤æˆåŠŸï¼Œæ–‡ä»¶æ•°é‡: {file_count}')
    else:
        print('âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨')
        exit(1)
        
except Exception as e:
    print(f'âŒ åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}')
    exit(1)
"; then
    log "âœ… åŠŸèƒ½æµ‹è¯•é€šè¿‡"
    RECOVERY_SUCCESS=true
else
    log "âŒ åŠŸèƒ½æµ‹è¯•å¤±è´¥"
    RECOVERY_SUCCESS=false
fi

# 9. æ¸…ç†æµ‹è¯•ç¯å¢ƒ
log "æ¸…ç†æµ‹è¯•ç¯å¢ƒ..."
docker rm -f trademaster-test postgres-test 2>/dev/null || true
docker volume rm trademaster-test-data 2>/dev/null || true
docker network rm trademaster-test 2>/dev/null || true
rm -rf "$TEST_DIR"

# 10. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
if [ "$RECOVERY_SUCCESS" = true ]; then
    log "ğŸ‰ ç¾éš¾æ¢å¤æµ‹è¯•å®Œå…¨æˆåŠŸ!"
    
    # è®°å½•æˆåŠŸçš„æ¢å¤æµ‹è¯•
    echo "$(date '+%Y-%m-%d %H:%M:%S') - æ¢å¤æµ‹è¯•æˆåŠŸ - å¤‡ä»½æ–‡ä»¶: $(basename $LATEST_BACKUP)" >> /var/log/recovery_test.log
    
    exit 0
else
    log "âŒ ç¾éš¾æ¢å¤æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å¤‡ä»½æµç¨‹"
    
    # å‘é€å‘Šè­¦
    if command -v mail >/dev/null 2>&1; then
        echo "TradeMasterç¾éš¾æ¢å¤æµ‹è¯•å¤±è´¥ï¼Œè¯·ç«‹å³æ£€æŸ¥å¤‡ä»½ç³»ç»Ÿ" | \
        mail -s "æ¢å¤æµ‹è¯•å¤±è´¥å‘Šè­¦" admin@example.com
    fi
    
    exit 1
fi
```

---

## ğŸ› ï¸ è¿ç»´ç®¡ç†æœ€ä½³å®è·µ

### ğŸ”„ è‡ªåŠ¨åŒ–è¿ç»´æµç¨‹

#### æ™ºèƒ½è¿ç»´è„šæœ¬
```bash
#!/bin/bash
# intelligent_ops.sh

set -euo pipefail

# é…ç½®
CONTAINER_NAME="trademaster-container"
LOG_FILE="/var/log/trademaster_ops.log"
ALERT_EMAIL="admin@example.com"
METRICS_API="http://localhost:9090/api/v1"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

# å¥åº·æ£€æŸ¥å‡½æ•°
health_check() {
    local service="$1"
    local endpoint="$2"
    local timeout="${3:-10}"
    
    if curl -sf --max-time "$timeout" "$endpoint" >/dev/null 2>&1; then
        return 0
    else
        return 1
    fi
}

# èµ„æºç›‘æ§å‡½æ•°
check_resources() {
    log "æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ..."
    
    # CPUä½¿ç”¨ç‡
    CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
    CPU_THRESHOLD=85.0
    
    if (( $(echo "$CPU_USAGE > $CPU_THRESHOLD" | bc -l) )); then
        log "è­¦å‘Š: CPUä½¿ç”¨ç‡è¿‡é«˜ ($CPU_USAGE%)"
        return 1
    fi
    
    # å†…å­˜ä½¿ç”¨ç‡
    MEM_USAGE=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
    MEM_THRESHOLD=85.0
    
    if (( $(echo "$MEM_USAGE > $MEM_THRESHOLD" | bc -l) )); then
        log "è­¦å‘Š: å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ($MEM_USAGE%)"
        return 1
    fi
    
    # ç£ç›˜ä½¿ç”¨ç‡
    DISK_USAGE=$(df /app/data | awk 'NR==2 {print $5}' | sed 's/%//')
    DISK_THRESHOLD=85
    
    if [ "$DISK_USAGE" -gt "$DISK_THRESHOLD" ]; then
        log "è­¦å‘Š: ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜ ($DISK_USAGE%)"
        return 1
    fi
    
    log "èµ„æºä½¿ç”¨æ­£å¸¸ - CPU: $CPU_USAGE%, å†…å­˜: $MEM_USAGE%, ç£ç›˜: $DISK_USAGE%"
    return 0
}

# æ€§èƒ½ä¼˜åŒ–å‡½æ•°
optimize_performance() {
    log "æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–..."
    
    # æ¸…ç†å®¹å™¨æ—¥å¿—
    docker exec "$CONTAINER_NAME" bash -c "
        find /var/log -name '*.log' -size +100M -exec truncate -s 50M {} \;
        find /tmp -type f -mtime +1 -delete
    " 2>/dev/null || true
    
    # Pythonåƒåœ¾å›æ”¶
    docker exec "$CONTAINER_NAME" python3 -c "
        import gc
        collected = gc.collect()
        print(f'æ¸…ç†äº† {collected} ä¸ªå¯¹è±¡')
    " 2>/dev/null || true
    
    # æ¸…ç†Dockerç¼“å­˜
    docker system prune -f >/dev/null 2>&1 || true
    
    log "æ€§èƒ½ä¼˜åŒ–å®Œæˆ"
}

# è‡ªåŠ¨æ‰©ç¼©å®¹å‡½æ•°
auto_scale() {
    local current_replicas=$(docker service ls --filter name=trademaster --format "{{.Replicas}}" 2>/dev/null | head -1 || echo "1")
    
    if [ -z "$current_replicas" ]; then
        log "æœªæ£€æµ‹åˆ°é›†ç¾¤æ¨¡å¼ï¼Œè·³è¿‡è‡ªåŠ¨æ‰©ç¼©å®¹"
        return 0
    fi
    
    # è·å–å¹³å‡è´Ÿè½½
    local avg_load=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//')
    local cpu_cores=$(nproc)
    local load_threshold=$(echo "$cpu_cores * 0.8" | bc)
    
    if (( $(echo "$avg_load > $load_threshold" | bc -l) )); then
        # é«˜è´Ÿè½½ï¼Œæ‰©å®¹
        local new_replicas=$((current_replicas + 1))
        if [ "$new_replicas" -le 5 ]; then
            log "é«˜è´Ÿè½½æ£€æµ‹ï¼Œæ‰©å®¹åˆ° $new_replicas ä¸ªå‰¯æœ¬"
            docker service update --replicas "$new_replicas" trademaster >/dev/null 2>&1 || true
        fi
    elif (( $(echo "$avg_load < $load_threshold * 0.3" | bc -l) )); then
        # ä½è´Ÿè½½ï¼Œç¼©å®¹
        local new_replicas=$((current_replicas - 1))
        if [ "$new_replicas" -ge 1 ]; then
            log "ä½è´Ÿè½½æ£€æµ‹ï¼Œç¼©å®¹åˆ° $new_replicas ä¸ªå‰¯æœ¬"
            docker service update --replicas "$new_replicas" trademaster >/dev/null 2>&1 || true
        fi
    fi
}

# å®‰å…¨æ£€æŸ¥å‡½æ•°
security_check() {
    log "æ‰§è¡Œå®‰å…¨æ£€æŸ¥..."
    
    # æ£€æŸ¥å®¹å™¨é…ç½®
    local security_issues=0
    
    # æ£€æŸ¥æ˜¯å¦ä»¥rootè¿è¡Œ
    if docker exec "$CONTAINER_NAME" whoami 2>/dev/null | grep -q "root"; then
        log "å®‰å…¨è­¦å‘Š: å®¹å™¨ä»¥rootç”¨æˆ·è¿è¡Œ"
        ((security_issues++))
    fi
    
    # æ£€æŸ¥ç«¯å£ç»‘å®š
    local open_ports=$(docker port "$CONTAINER_NAME" 2>/dev/null | grep "0.0.0.0" | wc -l)
    if [ "$open_ports" -gt 0 ]; then
        log "å®‰å…¨è­¦å‘Š: å­˜åœ¨ç»‘å®šåˆ°æ‰€æœ‰æ¥å£çš„ç«¯å£"
        ((security_issues++))
    fi
    
    # æ£€æŸ¥æ–‡ä»¶æƒé™
    local world_writable=$(docker exec "$CONTAINER_NAME" find /app -type f -perm /o+w 2>/dev/null | wc -l)
    if [ "$world_writable" -gt 0 ]; then
        log "å®‰å…¨è­¦å‘Š: å­˜åœ¨æ‰€æœ‰ç”¨æˆ·å¯å†™çš„æ–‡ä»¶"
        ((security_issues++))
    fi
    
    if [ "$security_issues" -eq 0 ]; then
        log "å®‰å…¨æ£€æŸ¥é€šè¿‡"
        return 0
    else
        log "å‘ç° $security_issues ä¸ªå®‰å…¨é—®é¢˜"
        return 1
    fi
}

# è‡ªåŠ¨ä¿®å¤å‡½æ•°
auto_repair() {
    local issue="$1"
    
    log "å°è¯•è‡ªåŠ¨ä¿®å¤: $issue"
    
    case "$issue" in
        "container_down")
            log "é‡å¯å®¹å™¨..."
            docker restart "$CONTAINER_NAME"
            sleep 30
            ;;
        "high_memory")
            log "æ¸…ç†å†…å­˜..."
            docker exec "$CONTAINER_NAME" python3 -c "
                import gc
                gc.collect()
                
                # æ¸…ç†ç¼“å­˜
                import os
                os.system('sync && echo 3 > /proc/sys/vm/drop_caches')
            " 2>/dev/null || true
            ;;
        "disk_full")
            log "æ¸…ç†ç£ç›˜ç©ºé—´..."
            docker exec "$CONTAINER_NAME" bash -c "
                find /tmp -type f -mtime +1 -delete
                find /app/logs -name '*.log' -mtime +7 -delete
                find /app/data -name '*.tmp' -delete
            " 2>/dev/null || true
            ;;
        "service_unhealthy")
            log "é‡æ–°éƒ¨ç½²æœåŠ¡..."
            if command -v docker-compose >/dev/null 2>&1; then
                docker-compose restart trademaster
            else
                docker restart "$CONTAINER_NAME"
            fi
            ;;
        *)
            log "æœªçŸ¥é—®é¢˜ï¼Œæ‰§è¡Œé€šç”¨ä¿®å¤..."
            docker restart "$CONTAINER_NAME"
            ;;
    esac
    
    log "è‡ªåŠ¨ä¿®å¤å®Œæˆ"
}

# ä¸»ç›‘æ§å¾ªç¯
main_monitor() {
    log "å¼€å§‹æ™ºèƒ½è¿ç»´ç›‘æ§..."
    
    while true; do
        # å®¹å™¨çŠ¶æ€æ£€æŸ¥
        if ! docker ps | grep -q "$CONTAINER_NAME"; then
            log "é”™è¯¯: å®¹å™¨æœªè¿è¡Œ"
            auto_repair "container_down"
            
            # éªŒè¯ä¿®å¤ç»“æœ
            sleep 30
            if docker ps | grep -q "$CONTAINER_NAME"; then
                log "å®¹å™¨è‡ªåŠ¨ä¿®å¤æˆåŠŸ"
            else
                log "å®¹å™¨è‡ªåŠ¨ä¿®å¤å¤±è´¥ï¼Œå‘é€å‘Šè­¦"
                send_alert "å®¹å™¨å¯åŠ¨å¤±è´¥" "å®¹å™¨ $CONTAINER_NAME æ— æ³•è‡ªåŠ¨ä¿®å¤"
            fi
        fi
        
        # å¥åº·æ£€æŸ¥
        if ! health_check "web" "http://localhost:8080/health"; then
            log "WebæœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥"
            auto_repair "service_unhealthy"
        fi
        
        # èµ„æºæ£€æŸ¥
        if ! check_resources; then
            # æ ¹æ®å…·ä½“é—®é¢˜è¿›è¡Œä¿®å¤
            if [ "$?" -eq 1 ]; then
                auto_repair "high_memory"
            fi
        fi
        
        # å®‰å…¨æ£€æŸ¥ï¼ˆæ¯å°æ—¶ä¸€æ¬¡ï¼‰
        if [ $(($(date +%M) % 60)) -eq 0 ]; then
            security_check || send_alert "å®‰å…¨æ£€æŸ¥å¤±è´¥" "å‘ç°å®‰å…¨é…ç½®é—®é¢˜"
        fi
        
        # æ€§èƒ½ä¼˜åŒ–ï¼ˆæ¯6å°æ—¶ä¸€æ¬¡ï¼‰
        if [ $(($(date +%H) % 6)) -eq 0 ] && [ $(date +%M) -eq 0 ]; then
            optimize_performance
        fi
        
        # è‡ªåŠ¨æ‰©ç¼©å®¹æ£€æŸ¥
        auto_scale
        
        # ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æŸ¥
        sleep 60
    done
}

# å‘Šè­¦å‘é€å‡½æ•°
send_alert() {
    local subject="$1"
    local message="$2"
    
    log "å‘é€å‘Šè­¦: $subject"
    
    # é‚®ä»¶å‘Šè­¦
    if command -v mail >/dev/null 2>&1; then
        echo "$message

æ—¶é—´: $(date)
ä¸»æœº: $(hostname)
å®¹å™¨: $CONTAINER_NAME" | mail -s "TradeMasterè¿ç»´å‘Šè­¦: $subject" "$ALERT_EMAIL"
    fi
    
    # Slackå‘Šè­¦ï¼ˆå¦‚æœé…ç½®äº†webhookï¼‰
    if [ -n "${SLACK_WEBHOOK:-}" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"TradeMasterè¿ç»´å‘Šè­¦: $subject\n$message\"}" \
            "$SLACK_WEBHOOK" >/dev/null 2>&1 || true
    fi
    
    # ä¼ä¸šå¾®ä¿¡å‘Šè­¦ï¼ˆå¦‚æœé…ç½®äº†webhookï¼‰
    if [ -n "${WECHAT_WEBHOOK:-}" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"msgtype\":\"text\",\"text\":{\"content\":\"TradeMasterè¿ç»´å‘Šè­¦: $subject\n$message\"}}" \
            "$WECHAT_WEBHOOK" >/dev/null 2>&1 || true