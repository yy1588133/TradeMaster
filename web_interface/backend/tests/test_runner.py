"""
æµ‹è¯•è¿è¡Œå’Œé…ç½®è„šæœ¬

æä¾›ä¾¿æ·çš„æµ‹è¯•æ‰§è¡Œã€è¦†ç›–ç‡ç»Ÿè®¡å’Œæµ‹è¯•æŠ¥å‘Šç”ŸæˆåŠŸèƒ½ã€‚
æ”¯æŒä¸åŒç±»å‹çš„æµ‹è¯•è¿è¡Œæ¨¡å¼å’Œæ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚
"""

import pytest
import sys
import os
import subprocess
from pathlib import Path


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹è¿è¡ŒTradeMasterå‰åç«¯é›†æˆæµ‹è¯•å¥—ä»¶...")
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    os.environ["TESTING"] = "1"
    os.environ["PYTHONPATH"] = str(Path(__file__).parent.parent)
    
    # æµ‹è¯•å‚æ•°
    args = [
        "-v",  # è¯¦ç»†è¾“å‡º
        "--tb=short",  # ç®€çŸ­çš„é”™è¯¯å›æº¯
        "--strict-markers",  # ä¸¥æ ¼æ ‡è®°æ£€æŸ¥
        "--disable-warnings",  # ç¦ç”¨è­¦å‘Š
        f"--rootdir={Path(__file__).parent}",
        "--asyncio-mode=auto",  # è‡ªåŠ¨å¼‚æ­¥æ¨¡å¼
        Path(__file__).parent  # æµ‹è¯•ç›®å½•
    ]
    
    return pytest.main(args)


def run_unit_tests():
    """åªè¿è¡Œå•å…ƒæµ‹è¯•"""
    print("ğŸ§ª è¿è¡Œå•å…ƒæµ‹è¯•...")
    
    args = [
        "-v",
        "-k", "not (integration or end_to_end)",
        Path(__file__).parent
    ]
    
    return pytest.main(args)


def run_integration_tests():
    """åªè¿è¡Œé›†æˆæµ‹è¯•"""
    print("ğŸ”— è¿è¡Œé›†æˆæµ‹è¯•...")
    
    args = [
        "-v", 
        "-k", "integration",
        Path(__file__).parent
    ]
    
    return pytest.main(args)


def run_end_to_end_tests():
    """åªè¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"""
    print("ğŸ¯ è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•...")
    
    args = [
        "-v",
        "-k", "end_to_end",
        Path(__file__).parent
    ]
    
    return pytest.main(args)


def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•...")
    
    args = [
        "-v",
        "-k", "performance",
        "--timeout=60",  # æ€§èƒ½æµ‹è¯•è¶…æ—¶60ç§’
        Path(__file__).parent
    ]
    
    return pytest.main(args)


def run_security_tests():
    """è¿è¡Œå®‰å…¨æµ‹è¯•"""
    print("ğŸ”’ è¿è¡Œå®‰å…¨å’Œé”™è¯¯å¤„ç†æµ‹è¯•...")
    
    args = [
        "-v",
        Path(__file__).parent / "test_security_and_errors.py"
    ]
    
    return pytest.main(args)


def run_with_coverage():
    """è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š"""
    print("ğŸ“Š è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š...")
    
    try:
        import coverage
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… coverage åŒ…: pip install coverage")
        return 1
    
    # å¯åŠ¨è¦†ç›–ç‡æ”¶é›†
    cov = coverage.Coverage(
        source=[str(Path(__file__).parent.parent / "app")],
        omit=[
            "*/tests/*",
            "*/test_*.py",
            "*/__init__.py",
            "*/venv/*",
            "*/env/*"
        ]
    )
    
    cov.start()
    
    try:
        # è¿è¡Œæµ‹è¯•
        exit_code = run_all_tests()
        
        # åœæ­¢è¦†ç›–ç‡æ”¶é›†
        cov.stop()
        cov.save()
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“ˆ è¦†ç›–ç‡æŠ¥å‘Š:")
        cov.report()
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        htmlcov_dir = Path(__file__).parent / "htmlcov"
        cov.html_report(directory=str(htmlcov_dir))
        print(f"ğŸ“‹ HTMLè¦†ç›–ç‡æŠ¥å‘Šå·²ç”Ÿæˆ: {htmlcov_dir}/index.html")
        
        return exit_code
        
    except Exception as e:
        print(f"âŒ è¦†ç›–ç‡æµ‹è¯•å¤±è´¥: {e}")
        return 1


def run_specific_test_file(file_name: str):
    """è¿è¡ŒæŒ‡å®šçš„æµ‹è¯•æ–‡ä»¶"""
    test_file = Path(__file__).parent / file_name
    
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return 1
    
    print(f"ğŸª è¿è¡Œæµ‹è¯•æ–‡ä»¶: {file_name}")
    
    args = ["-v", str(test_file)]
    return pytest.main(args)


def run_tests_with_markers(*markers):
    """è¿è¡Œå¸¦æœ‰ç‰¹å®šæ ‡è®°çš„æµ‹è¯•"""
    if not markers:
        print("âŒ è¯·æŒ‡å®šè‡³å°‘ä¸€ä¸ªæµ‹è¯•æ ‡è®°")
        return 1
    
    marker_expr = " or ".join(markers)
    print(f"ğŸ·ï¸  è¿è¡Œæ ‡è®°ä¸º '{marker_expr}' çš„æµ‹è¯•...")
    
    args = [
        "-v",
        "-m", marker_expr,
        Path(__file__).parent
    ]
    
    return pytest.main(args)


def generate_test_report():
    """ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š"""
    print("ğŸ“ ç”Ÿæˆè¯¦ç»†æµ‹è¯•æŠ¥å‘Š...")
    
    try:
        import pytest_html
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… pytest-html åŒ…: pip install pytest-html")
        return 1
    
    report_file = Path(__file__).parent / "test_report.html"
    
    args = [
        "-v",
        "--html", str(report_file),
        "--self-contained-html",
        Path(__file__).parent
    ]
    
    exit_code = pytest.main(args)
    
    if exit_code == 0:
        print(f"ğŸ“‹ æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    return exit_code


def run_benchmark_tests():
    """è¿è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•"""
    print("ğŸƒ è¿è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•...")
    
    try:
        import pytest_benchmark
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… pytest-benchmark åŒ…: pip install pytest-benchmark")
        return 1
    
    args = [
        "-v",
        "--benchmark-only",
        "--benchmark-sort=mean",
        "-k", "benchmark",
        Path(__file__).parent
    ]
    
    return pytest.main(args)


def validate_test_environment():
    """éªŒè¯æµ‹è¯•ç¯å¢ƒé…ç½®"""
    print("ğŸ” éªŒè¯æµ‹è¯•ç¯å¢ƒ...")
    
    required_packages = [
        "pytest",
        "pytest-asyncio", 
        "httpx",
        "sqlalchemy",
        "fastapi"
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package.replace("-", "_"))
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print(f"âŒ ç¼ºå°‘å¿…éœ€çš„åŒ…: {', '.join(missing_packages)}")
        print("è¯·è¿è¡Œ: pip install " + " ".join(missing_packages))
        return False
    
    # æ£€æŸ¥æµ‹è¯•æ•°æ®åº“
    test_db_path = Path(__file__).parent / "test.db"
    if test_db_path.exists():
        print("ğŸ—‚ï¸  å‘ç°ç°æœ‰æµ‹è¯•æ•°æ®åº“ï¼Œå°†è‡ªåŠ¨æ¸…ç†")
    
    print("âœ… æµ‹è¯•ç¯å¢ƒéªŒè¯å®Œæˆ")
    return True


def clean_test_artifacts():
    """æ¸…ç†æµ‹è¯•äº§ç”Ÿçš„ä¸´æ—¶æ–‡ä»¶"""
    print("ğŸ§¹ æ¸…ç†æµ‹è¯•äº§ç”Ÿçš„æ–‡ä»¶...")
    
    patterns = [
        "test.db*",
        "*.pyc",
        "__pycache__",
        ".pytest_cache",
        "htmlcov",
        "test_report.html",
        ".coverage"
    ]
    
    base_dir = Path(__file__).parent
    
    for pattern in patterns:
        for item in base_dir.rglob(pattern):
            try:
                if item.is_file():
                    item.unlink()
                    print(f"ğŸ—‘ï¸  åˆ é™¤æ–‡ä»¶: {item}")
                elif item.is_dir():
                    import shutil
                    shutil.rmtree(item)
                    print(f"ğŸ—‘ï¸  åˆ é™¤ç›®å½•: {item}")
            except (OSError, PermissionError) as e:
                print(f"âš ï¸  æ— æ³•åˆ é™¤ {item}: {e}")
    
    print("âœ… æ¸…ç†å®Œæˆ")


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    if len(sys.argv) < 2:
        print("""
ğŸ§ª TradeMaster æµ‹è¯•å¥—ä»¶è¿è¡Œå™¨

ä½¿ç”¨æ–¹æ³•:
  python test_runner.py <command> [args]

å¯ç”¨å‘½ä»¤:
  all                    è¿è¡Œæ‰€æœ‰æµ‹è¯•
  unit                   åªè¿è¡Œå•å…ƒæµ‹è¯•
  integration            åªè¿è¡Œé›†æˆæµ‹è¯•
  e2e                    åªè¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
  performance            åªè¿è¡Œæ€§èƒ½æµ‹è¯•
  security               åªè¿è¡Œå®‰å…¨æµ‹è¯•
  coverage               è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
  report                 ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š
  benchmark              è¿è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•
  file <filename>        è¿è¡ŒæŒ‡å®šæµ‹è¯•æ–‡ä»¶
  marker <markers...>    è¿è¡Œå¸¦æœ‰æŒ‡å®šæ ‡è®°çš„æµ‹è¯•
  validate               éªŒè¯æµ‹è¯•ç¯å¢ƒ
  clean                  æ¸…ç†æµ‹è¯•äº§ç”Ÿçš„æ–‡ä»¶

ç¤ºä¾‹:
  python test_runner.py all
  python test_runner.py file test_trademaster_integration.py
  python test_runner.py marker slow integration
  python test_runner.py coverage
        """)
        return 0
    
    command = sys.argv[1].lower()
    
    # éªŒè¯ç¯å¢ƒï¼ˆé™¤äº†cleanå’Œvalidateå‘½ä»¤ï¼‰
    if command not in ["clean", "validate"]:
        if not validate_test_environment():
            return 1
    
    if command == "all":
        return run_all_tests()
    elif command == "unit":
        return run_unit_tests()
    elif command in ["integration", "int"]:
        return run_integration_tests()
    elif command in ["e2e", "end_to_end"]:
        return run_end_to_end_tests()
    elif command in ["performance", "perf"]:
        return run_performance_tests()
    elif command in ["security", "sec"]:
        return run_security_tests()
    elif command == "coverage":
        return run_with_coverage()
    elif command == "report":
        return generate_test_report()
    elif command in ["benchmark", "bench"]:
        return run_benchmark_tests()
    elif command == "file":
        if len(sys.argv) < 3:
            print("âŒ è¯·æŒ‡å®šæµ‹è¯•æ–‡ä»¶å")
            return 1
        return run_specific_test_file(sys.argv[2])
    elif command == "marker":
        if len(sys.argv) < 3:
            print("âŒ è¯·æŒ‡å®šè‡³å°‘ä¸€ä¸ªæµ‹è¯•æ ‡è®°")
            return 1
        return run_tests_with_markers(*sys.argv[2:])
    elif command == "validate":
        return 0 if validate_test_environment() else 1
    elif command == "clean":
        clean_test_artifacts()
        return 0
    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)